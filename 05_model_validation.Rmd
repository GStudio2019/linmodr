---
title: "Диагностика линейных моделей"
subtitle: "Линейные модели..."
author: "Марина Варфоломеева, Вадим Хайтов"
output:
  beamer_presentation:
    colortheme: beaver
    highlight: tango
    includes:
      in_header: ./includes/header.tex
    pandoc_args:
    - --latex-engine=xelatex
    - -V fontsize=10pt
    - -V lang=russian
    slide_level: 2
    theme: default
    toc: no
institute: "Кафедра Зоологии беспозвоночных, Биологический факультет, СПбГУ"
---

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
# output options
options(width = 70, scipen = 6, digits = 3)
library(knitr)
# chunk default options
# opts_chunk$set(fig.align='center', tidy = FALSE, fig.width = 7, fig.height = 3, warning = FALSE)
opts_chunk$set(fig.show='hold', size='footnotesize', comment="#", warning=FALSE, message=FALSE, dev='cairo_pdf', fig.height=2.5, fig.width=7.7)
library("extrafont")
```

## Диагностика линейных моделей

- Анализ остатков
    - Проверка на наличие влиятельных наблюдений
    - Проверка условий применимости линейных моделей

### Вы сможете

- перечислить условия применимости линейной регрессии
- идентифицировать основные нарушения условий применимости линейной регрессии по паттернам на графиках остатков
- написать код на языке R, который позволяет нарисовать диагностические графики остатков для линейной регрессии


## Зачем нужна диагностика модели? Разве тестов было недостаточно?

```
dat <- read.table('http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/
Hidden_Images/orly_owl_files/orly_owl_Lin_4p_5_flat.txt'
```

```{r echo=FALSE}
dat <- read.table('http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/orly_owl_files/orly_owl_Lin_4p_5_flat.txt')
```

```{r}
fit <- lm(V1 ~ V2 + V3 + V4 + V5 - 1, data = dat)
coef(summary(fit))
```

Все достоверно? Пишем статью?

\pause

Постройте график зависимости остатков от предсказанных значений при помощи этого кода

```{r bird, eval=FALSE, purl=FALSE}
library(car)
residualPlot(fit, pch = ".")
```

## Oh, really?

```{r bird, eval=TRUE, fig.width=3.7, fig.height=4, purl=TRUE, out.height='6cm', fig.align='center'}
```

\tiny{http://www4.stat.ncsu.edu/~stefanski/NSF\_Supported/Hidden\_Images/stat\_res\_plots.html}

## Анализ остатков линейных моделей

### Проверка на наличие влиятельных наблюдений

### Проверка условий применимости линейных моделей

- Линейная связь между зависимой перменной ($Y$) и предикторами ($X$)
- Независимость значений $Y$ друг от друга
- Нормальное распределение $Y$ для каждого уровня значений $X$
- Гомогенность дисерсий $Y$ для каждого уровня значений $X$
- Отсутствие коллинеарности предикторов (для можественной регрессии)


## С этим примером мы познакомились в прошлый раз: IQ и размеры мозга

Зависит ли уровень интеллекта от размера головного мозга? (Willerman et al. 1991)

\columnsbegin
\column{0.48\textwidth}
\begin{figure}
\centering
\includegraphics{./images/MRI-Scan_03_11-by_bucaorg(Paul_Burnett)_no_Flickr.jpg}
\caption{\footnotesize{}}
\end{figure}


\column{0.48\textwidth}
Было исследовано 20 девушек и 20 молодых людей.

У каждого индивида измеряли:

- вес
- рост
- размер головного мозга (количество пикселей на изображении ЯМР сканера)
- уровень интеллекта (различные IQ тесты)



\columnsend

\tiny{Пример: Willerman, L., Schultz, R., Rutledge, J. N., and Bigler, E. (1991), "In Vivo Brain Size and Intelligence", Intelligence, 15, p.223--228. 
Данные: \href{http://lib.stat.cmu.edu/DASL}{"The Data and Story Library"} 
Фото: \href{https://flic.kr/p/c45eZ3}{Scan\_03\_11} by bucaorg(Paul\_Burnett) on Flickr}

## Вспомним, на чем мы остановились

Не забудьте войти в вашу директорию для матметодов при помощи `setwd()`

```{r eval=FALSE}
# Данные можно загрузить с сайта
library(downloader)
# в рабочем каталоге создаем суб-директорию для данных
if(!dir.exists("data")) dir.create("data")
# скачиваем файл
download(
  url = "https://varmara.github.io/linmodr/data/IQ_brain.csv", 
  destfile = "data/IQ_brain.csv")
```
### Подберем модель
```{r}
brain <- read.csv("data/IQ_brain.csv", header = TRUE)
brain_model <- lm(PIQ ~ MRINACount, data = brain)
coef(summary(brain_model))
```

# Проверка на наличие влиятельных наблюдений

## Влиятельные наблюдения --- это...

наблюдения, которые вносят слишком большой вклад в оценку парметров (коэффициентов) модели.

\columnsbegin
\column{0.48\textwidth}
\centering
\includegraphics{./images/leverage.png}

\footnotesize{Из кн. Quinn, Keugh, 2002}


\column{0.48\textwidth}
Учет каких из этих точек повлияет на ход регрессии и почему?
\pause

- Точка 1 почти не повлияет, т.к. у нее маленький остаток, хоть и большой $X$
- Точка 2 почти не повлияет, т.к. ее $X$ близок к среднему, хоть и большой остаток
- Точка 3 повлияет сильно, т.к. у нее не только большой остаток, но и большой $X$


\columnsend


## Типы остатков

### "Сырые" остатки

$\varepsilon_i = y_i - \hat{y_i}$

\pause

### Пирсоновские остатки

$p_i = \frac{\varepsilon_i}{\sqrt{Var(\hat{y_i})}}$,
где $\sqrt{Var(\hat{y_i})}$ --- это cтандартное отклонение предсказанных значений

легко сравнивать, т.к. стандартизованы

\pause

### Стьюдентовские остатки

$s_i = \frac{p_i}{\sqrt{1 - h_{ii}}} = \frac{\varepsilon_i}{\sqrt{Var(\hat{y_i})(1-h_{ii})}}$,  
где $h_{ii}$ --- "сила воздействия" отдельных наблюдений (leverage)

легко сравнивать, т.к. стандартизованы и учитывают влияние наблюдений


## Воздействие точек $h_{ii}$ (leverage)

### показывает, насколько каждое значение $x_i$ влияет на ход линии регрессии, то есть на $\hat{y_i}$

\columnsbegin

\column[b]{0.55\textwidth}
\centering
\includegraphics[height=4cm]{images/leverage.png}

\tiny{Из кн. Quinn, Keough, 2002}

\includegraphics[height=2.5cm]{images/seasaw-Weighing-Machine-by-neys-fadzil-on-Flickr.jpg}

\tiny{Weighing Machine by neys fadzil on Flickr}



\column[b]{0.45\textwidth}

- Точки, располагающиеся дальше от $\bar{x}$, оказывают более сильное влияние на $\hat{y_i}$  
- Эта величина, в норме, варьирует в промежутке от $1/n$ до 1  
- Если  $h_{ii} > 2(p/n)$, то надо внимательно посмотреть на данное значение (p --- число параметров, n --- объем выборки)



\columnsend

## Расстояние Кука (Cook's distance)

### описывает, как повлияет на модель удаление данного наблюдения

$$D_i = \frac{\sum{(\hat{y_j}-\hat{y}_{j(i)})^2}}{p \cdot MSE} \large( \frac {h_{ii}} {1 - h_{ii}} \large)$$ 

- $\hat{y_j}$ - значение предсказанное полной моделью
- $\hat{y}_{j(i)}$ - значение, предказанное моделью, построенной без учета $i$-го значения предиктора
- $p$ - количество параметров в модели
- $MSE$ - среднеквадратичная ошибка модели ($\hat\sigma^2$)
- $h_{ii}$ --- "сила воздействия" отдельных наблюдений (leverage)

\pause

Расстояние Кука зависит одновременно от величины остатков и "силы воздействия" наблюдений.

Статистических тестов для $D_i$ нет, но можно использовать один из двух условных порогов. Наблюдение является выбросом (outlier), если:

- $D_i > 1$ 
- $D_i > 4/(N − k − 1)$ (N - объем выборки, k - число предикторов)


## Проверяем наличие влиятельных наблюдений в `brain_model`

График расстояния Кука.  
Значения приведены в том же порядке, что и в исходных данных.

```{r}
plot(brain_model, which = 4)
```

Второй вариант --- построить обычный график остатков, и отметить на нем расстояния Кука.

## Чтобы построить график остатков с расстояниями Кука нам понадобятся данные.

Извлечем из результатов сведения для анализа остатков при помощи функции `fortify()` из пакета `{ggplot2}`

```{r}
library(ggplot2)
brain_diag <- fortify(brain_model)
head(brain_diag, 2)
```

\pause

- `.hat` --- "сила воздействия" данного наблюдения (_leverage_)  
- `.cooksd` --- расстояние Кука   
- `.fitted` --- предсказанные значения   
- `.resid` --- остатки
- `.stdresid` --- стандартизованные остатки  

## Задание

Используя данные из датафрейма `brain_diag`,  
постройте график зависимости стандартизированных остатков модели `brain_model` от предсказанных значений.

Сделайте так, чтобы размер точек изменялся в зависимости от значения расстояния Кука.



## Решение

```{r purl=FALSE}
theme_set(theme_bw()) # устанавливаем тему (не обязательно)
ggplot(data = brain_diag, aes(x = .fitted, y = .stdresid, size = .cooksd)) + 
  geom_point() + geom_hline(aes(yintercept = 0))
```

Что мы видим?

\pause

- Большая часть стандартизованных остатков в пределах двух стандартных отклонений
- Есть одно влиятельное наблюдение, которое нужно проверить, но сила его влияния невелика (расстояние Кука < 1)
- Среди остатков нет тренда, но, возможно, есть иной паттерн...


## Добавим линию loess-сглаживания на график

```{r}
ggplot(data = brain_diag, aes(x = .fitted, y = .stdresid, size = .cooksd)) +
  geom_point() +  geom_hline(yintercept = 0) + 
  geom_smooth(method="loess", se=FALSE) 
```

Чем мог быть вызван такой странный паттерн?

\pause

- Неучтенная переменная --- добавляем в модель
- Нелинейная зависимость --- используем GAM, нелинейную регрессию и т.д.


## Что делать с наблюдениями-выбросами?

### Удалить?

__Осторожно!__ Нельзя удалять выбросы только на основе такого диагноза. Задача диагностики --- заставить вас искать причины такого поведения данных. 
Удалять следует только очевидные ошибки в наблюдениях.

### Трансформировать?

Некоторые виды трансформаций

Трансформация  |  Формула  
------------- | -------------   
степень -2 | $1/x^2$
степень -1 | $1/x$
степень -0.5  | $1/\sqrt{x}$
степень 0.5 | $\sqrt{x}$
логарифмирование | $log(x)$  

# Условия применимости линейных моделей (Assumptions)

## 1. Линейность связи

Нелинейные зависимости не всегда видны на исходных графиках в осях Y vs X

Они становятся лучше заметны на графиках рассеяния остатков (Residual plots)

```{r echo=FALSE, purl=FALSE}
library(gridExtra)

set.seed(92387)
x <- rnorm(100, 10, 3)
y <- (x^2.4) + rnorm(100, 0, 100)
pl_1 <- ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point() 

lm1 <- lm(y ~ x)

pl_1res <- ggplot(data.frame(fit=fitted(lm1), res=residuals(lm1)), aes(x=fit, y=res)) + geom_point() + geom_hline(yintercept=0) + xlab("Fitted") + ylab("Residuals")


x2 <- runif(100, 1, 10)
y2 <- sin(x2) + 1.8*x2 + rnorm(100)
pl_2 <- ggplot(data.frame(x=x2, y=y2), aes(x=x, y=y)) + geom_point() 

lm2 <- lm(y2 ~ x2)
pl_2res <- ggplot(data.frame(fit=fitted(lm2), res=residuals(lm2)), aes(x=fit, y=res)) + geom_point() + geom_hline(yintercept=0) + xlab("Fitted") + ylab("Residuals") 

grid.arrange(pl_1, pl_2, pl_1res, pl_2res)
```

### Проверка на линейность связи

- График зависимости $Y$ от $x$ (для множественной регрессии - от всех $x$)
- График остатков от предсказанных значений

## Что делать, если связь нелинейна?  

- Добавить неучтенные переменные
- Добавить взаимодействие переменных
- Применить линеаризующее преобразование (Осторожно!)
- Применить обобщенную линейную модель с другой функцией связи (об этом позже)
- Построить аддитивную модель (если достаточно наблюдений по $x$)
- Построить нелинейную модель (если известна форма зависимости)


## Пример линеаризующего преобразования   

```{r echo=FALSE, purl=FALSE}
set.seed(29834)
x <- runif(100, 2, 5)
y <- (2^(2*x)) + rnorm(100, 0, 70)

pl_raw <- ggplot(data.frame(x=(x), y=(y)), aes(x=x, y=y)) + geom_point() + geom_smooth(method = "lm", alpha = 0.7) 

pl_log <- ggplot(data.frame(x= (x), y=log(y)), aes(x=x, y=y)) + geom_point() + geom_smooth(method = "lm", alpha = 0.7) + ylab("Log (y)")

grid.arrange(pl_raw, pl_log, ncol=2)

```

\pause

__Осторожно!__ При таком преобразовании вы рискуете изучить не то, что хотели. Матожидание логарифма величины (как при трансформации) не то же самое, что логарифм матожидания величины (как при использовании обобщенной линейной модели с логарифмической функцией связи). Но об этом --- позже.

## 2. Независимость $Y$ друг от друга

Каждое значение $Y_i$ должно быть независимо от любого другого $Y_j$ 

Это нужно контролировать на этапе планирования сбора материала 

* Наиболее частые источники зависимостей: 
    + псевдоповторности (повторно измеренные объекты)
    + неучтенные переменные
    + временные автокорреляции (если данные - временной ряд)
    + пространственные автокорреляции (если пробы взяты в разных местах)
    + и т.п.

\pause

Взаимозависимости можно заметить на графиках остатков:

- остатки vs. предсказанные значения
- остатки vs. переменные в модели
- остатки vs. переменные не в модели

## Нарушение условия независимости: Неучтенная переменная

```{r echo=FALSE, purl=FALSE, fig.height=4}
set.seed(239874)
x1 <- runif(100, 20, 50)
x2 <- runif(100, 8, 22)
y <- 21 + 2*x1 + 0.5*x2 + rnorm(100, 0, 10)
NewData1 <- data.frame(y = y, x1 = x1, x2 = x2)

mod1 <- lm(y~x1)
gg_lm1 <- ggplot(NewData1, aes(x=x1, y=y)) + geom_point() + geom_smooth(method = "lm", alpha = 0.7) + xlab("X1") + ggtitle("Y ~ X1")
gg_res1 <- ggplot(data.frame(fit = fitted(mod1), res = residuals(mod1, type = "pearson")), aes(x = fit, y = res)) + geom_point() + geom_smooth(se = FALSE) + geom_hline(yintercept = 0) + xlab("Fitted") + ylab("Residuals")
gg_res2 <- ggplot(data.frame(fit = fitted(mod1), res = residuals(mod1, type = "pearson")), aes(x = x2, y = res)) + geom_point() + geom_smooth(se = FALSE) + geom_hline(yintercept = 0) + xlab("X2") + ylab("Residuals")


mod2 <- lm(y~x1+x2)
NewData2 <- data.frame(x1 = seq(min(x1), max(x1), length.out = 10),
                      x2 = mean(x2))
NewData2$y <- predict(mod2, newdata = NewData2)
gg_lm2 <- ggplot(NewData2, aes(x= x1, y = y)) + geom_point(data = NewData1, aes(x = x1, y = y)) + geom_line(colour = "blue", size = 1) + xlab("X1") + ggtitle("Y ~ X1 + X2")
gg_res3 <- ggplot(data.frame(fit = fitted(mod2), res = residuals(mod2, type = "pearson")), aes(x = fit, y = res)) + geom_point() + geom_smooth(se = FALSE) + geom_hline(yintercept = 0) + xlab("Fitted") + ylab("Residuals")
gg_res4 <- ggplot(data.frame(fit = fitted(mod2), res = residuals(mod2, type = "pearson")), aes(x = x2, y = res)) + geom_point() + geom_smooth(se = FALSE) + geom_hline(yintercept = 0) + xlab("X2") + ylab("Residuals")

grid.arrange(gg_lm1, gg_lm2, gg_res1, gg_res3, gg_res2, gg_res4, ncol=2)
```

Если в модели не учтена переменная $X2$ (слева), внешне все нормально (только остатки большие), но если построить график зависимости остатков от $X2$.

Если $X2$ учесть (справа) --- остатки становятся меньше, зависимость остатков от $X2$ исчезает.

## Нарушение условия независимости: Автокорреляция

В данном случае, наблюдения --- это временной ряд. 

```{r echo=FALSE, purl=FALSE}
x3 <- seq(1, 100, 1)
  
y3 <-  diffinv(rnorm(99)) + rnorm(100, 0, 2)

y3 <- y3[1:100]
pl_3 <- ggplot(data.frame(x=x3, y=y3), aes(x=x, y=y)) + geom_point() + geom_smooth(method = "lm", alpha = 0.7)

lm3 <- lm(y3 ~ x3)

pl_3res <- ggplot(data.frame(fit=fitted(lm3), res=residuals(lm3)), aes(x=fit, y=res)) + geom_point() + geom_smooth(se = FALSE) + geom_hline(yintercept=0) + xlab("Fitted") + ylab("Residuals")

grid.arrange(pl_3, pl_3res, nrow=2)
```

На графиках остатков четко видно, что остатки не являются независимыми.

## Проверка на автокорреляцию

Проверка на автокорреляцию нужна если данные это временной ряд, или если известны координаты проб.

Способы проверки временной автокорреляции (годятся, если наблюдения в ряду расположены через равные интервалы):

- График автокорреляционной функции остатков (ACF-plot) покажет корреляции с разными лагами.
- Критерий Дарбина-Уотсона (значимость автокорреляции 1-го порядка).

Для проверки пространственных автокорреляций

- вариограмма
- I Морана (Moran's I)

## Что делать, если у вас нарушено условие независимости значений?

Выбор зависит от обстоятельств. Вот несколько возможных вариантов.

+ псевдоповторности
    - избавляемся от псевдоповторностей, вычислив среднее
    - подбираем модель со случайным фактором
+ неучтенные переменные
    - включаем в модель (если возможно)
+ временные автокорреляции
    - моделируем автокорреляцию
    - подбираем модель со случайным фактором
+ пространственные автокорреляции
    - моделируем пространственную автокорреляцию
    - делим на пространственные блоки и подбираем модель со случайным фактором


## 3. Нормальное распределение $Y$ (для каждого уровня значений $X$) 

<img src="figure/Zuur.png" width="400" height="300" >   

<img src="figure/Normality.png" width="400" height="200" > 

Это условие невозможно проверить "влоб", т.к. обычно каждому $X$ сообветствует лишь небольшое число $Y$ 

Если $Y$ это нормально распределенная случайная величина

$$Y_i \in N(\mu_{y_i}, \sigma^2)$$

и мы моделируем ее как 

$$Y_i \sim b_0 + b_1x_{1i} + \cdots + \varepsilon_i$$  

то остатки от этой модели --- тоже нормально распределенная случайная величина 

$$\varepsilon_i \in N(\mu_{y_i}, \sigma^2)$$

Т.е. выполнение этого условия можно оценить по поведению случайной части модели.

## Проверка нормальности распределения остатков

Есть формальные тесты, но:

- у формальных тестов тоже есть свои условия применимости
- при больших выборках формальные тесты покажут, что значимы даже небольшие отклонения от нормального распределения
- тесты, которые используются в линейной регрессии, устойчивы к небольшим отклонениям от нормального распределения

Лучший способ проверки --- квантильный график остатков.

## Квантильный график остатков

_Квантиль_ - значение, которое заданная случайная величина не превышает с фиксированной вероятностью.       

Если точки - это реализации случайной величины из $N(0, \sigma^2)$, то они должны лечь вдоль прямой $Y=X$. Если это стьюдентизированные остатки --- то используются квантили t-распределения

```{r}
qqPlot(brain_model) # из пакета car
```

## Аналогичный график при помощи `ggplot2`

```{r, fig.height=3, fig.align='center', tidy=TRUE}
mean_val <- mean(brain_diag$.stdresid)
sd_val <- sd(brain_diag$.stdresid)
ggplot(brain_diag, aes(sample = .stdresid)) + geom_point(stat = "qq") + geom_abline(intercept = mean_val, slope = sd_val)
```

## Что делать, если остатки распределены не нормально?

Зависит от причины

- Нелинейная связь?
    - Построить аддитивную модель (если достаточно наблюдений по $x$)
    - Построить нелинейную модель (если известна форма зависимости)
- Неучтенные переменные?
    - добавляем в модель
- Зависимая переменная распределена по-другому?
    - трансформируем данные (неудобно)
    - подбираем модель с другим распределением остатков (обобщенную линейную модель)

## 4. Постоянство дисперсии (гомоскедастичность)

Это самое важное условие, поскольку многие тесты чувствительны к гетероскедастичности.     

```{r echo=FALSE, purl=FALSE}
N <- 300
b_0 <- 0.2 
b_1 <- 5

set.seed(123456)
x <- rnorm(N, 10, 3)
eps_1 <- rnorm(N, 0, 10)
y_1 <- b_0 + b_1*x + eps_1

# |v|^(2*t), t = 0.7
h <- function(x) x^(2*0.7) 
eps_2 <- rnorm(N, 0, h(x))
y_2 <- b_0 + b_1*x + eps_2
dat <- data.frame(x, y_1, y_2)
dat$log_y <- log(y_2)

pl_hom <- ggplot(dat, aes(x = x, y = y_1)) + geom_point() + geom_smooth(method = "lm", alpha = 0.7) + ggtitle("Гомоскедастичность") + ylab("Y")
pl_heter <- pl_hom + aes(y = y_2) + ggtitle("Гетероскедастичность") + ylab("Y")

dat_diag_1 <- fortify(lm(y_1 ~ x, data = dat))
dat_diag_2 <- fortify(lm(y_2 ~ x, data = dat))

pl_hom_resid <- ggplot(dat_diag_1, aes(x = .fitted, y = .stdresid)) + geom_point() + geom_smooth(se=FALSE)
pl_heter_resid <- pl_hom_resid %+% dat_diag_2

grid.arrange (pl_hom, pl_heter, 
              pl_hom_resid, pl_heter_resid, 
              ncol=2, heights = c(0.55, 0.45))

```

## Проверка постоянства дисперсий

Есть формальные тесты (тест Бройша-Пагана, тест Кокрана), но:

- у формальных тестов тоже есть свои условия применимости, и многие сами неустойчивы к гетероскедастичности
- при больших выборках формальные тесты покажут, что значима даже небольшая гетероскедастичность

Лучший способ проверки --- график остатков.

## Проверка на гетероскедастичность

Мы уже строили график остатков в `ggplot2`

```{r}
ggplot(data = brain_diag,
       aes(x = .fitted, y = .stdresid)) +
  geom_point() + geom_hline(yintercept = 0) 
```

## Проверка на гетероскедастичность

Можем построить аналогичный график остатков средствами пакета `car`

```{r fig.height = 4, out.height='4cm'}
residualPlot(brain_model)
```

\pause

- Гетерогенность дисперсий не выражена.

## Что делать если вы столкнулись с гетероскедастичностью?

\columnsbegin

\column{.5\textwidth}

```{r out.width='6cm', out.height='8cm', fig.width=4.8, fig.height=6.4, echo=FALSE, purl=FALSE}
dat_diag2 <- fortify(lm(log_y~x, data=dat))
pl_heter2 <- ggplot(dat, aes(x=x, y=log_y)) + geom_point() + geom_smooth(method = "lm", alpha = 0.7)
pl_heter_resid2 <- ggplot(dat_diag2, aes(x = .fitted, y = .stdresid)) + geom_point() + geom_smooth(se=FALSE)
pl_heter <- pl_heter + ggtitle("No transformation")
pl_heter2 <- pl_heter2 + ggtitle("Log transformed Y")
grid.arrange (pl_heter, pl_heter2,  pl_heter_resid, pl_heter_resid2,  nrow=2)
```

\column{.5\textwidth}

Трансформация может помочь...

Но на самом деле, нужно смотреть на причину гетерогенности

- Неучтенные переменные
    - добавляем в модель
- Зависимая переменная распределена по-другому
    - трансформируем данные (неудобно)
    - подбираем модель с другим распределением остатков (обобщенную линейную модель)
- Моделируем гетерогенность дисперсии.

\columnsend

## Некоторые распространенные паттерны на графиках остатков


\columnsbegin

\column{0.5\textwidth}
\includegraphics[height=8cm]{images/Residuals.png}

\tiny{Bз кн. Logan, 2010, стр. 174}

\column{0.48\textwidth}

\pause

- a) Условия применимости соблюдаются. Модель хорошая
- b) Клиновидный паттерн. Есть гетероскедастичность. Модель плохая
- c) Остатки рассеяны равномерно, но модель неполна. Нужны дополнительные предикторы. Модель можно улучшить
- d) Нелинейный паттерн сохранился. Линейная модель использована некорректно. Модель плохая

\columnsend

## Задание

Выполните три блока кода (см. код лекции). 

Какие нарушения условий применимости линейных моделей здесь наблюдаются?

## Задание, блок 1

```{r}
set.seed(12345)
x1 <- seq(1, 100, 1)
y1 <-  diffinv(rnorm(99)) + rnorm(100, 0.2, 2)
dat1 = data.frame(x1, y1)
ggplot(dat1, aes(x = x1, y = y1)) + geom_point()+ 
  geom_smooth(method="lm", alpha = 0.7)
```

## Решение, блок 1

```{r fig.show='hold', purl=FALSE}
mod1 <- lm(y1 ~ x1, data = dat1)
op <- par(mfrow = c(1, 3)) # располагаем картинки в 3 колонки
plot(mod1, which = 4)          # Расстояние Кука
residualPlot(mod1)             # График остатков
qqPlot(mod1)                   # Квантильный график остатков
par(op)                    # возвращаем старые графические параметры
```

\pause

- Выбросов нет
- Зависимость нелинейна
- Остатки не подчиняются нормальному распределению


## Задание, блок 2

```{r}
  set.seed(12345)
  x2 <- runif(1000, 1, 100)
  b_0 <- 100;  b_1 <- 20
  h <- function(x) x^0.5
  eps <- rnorm(1000, 0, h(x2))
  y2 <- b_0 + b_1 * x2 + eps
  dat2 <- data.frame(x2, y2)
  ggplot(dat2, aes(x = x2, y = y2)) + geom_point() + geom_smooth(method = "lm")
```

## Решение, блок 2

```{r fig.show='hold', purl=FALSE}
mod2 <- lm(y2 ~ x2, data = dat2)
op <- par(mfrow = c(1, 3))
plot(mod2, which = 4)
residualPlot(mod2)
qqPlot(mod2)
par(op)
```

\pause

- Выбросов нет
- Гетерогенность дисперсий
- Остатки не подчиняются нормальному распределению

## Задание, блок 3

```{r}
set.seed(2309587)
x3 <- rnorm(100, 50, 10)
b_0 <- 100; b_1 <- 20; eps <- rnorm(100, 0, 100)
y3 <- b_0 + b_1*x3 + eps
y3[100] <- 1000; x3[100] <- 95; y3[99] <- 1300; x3[99] <- 90; y3[98] <- 1500; x3[98] <- 80
dat3 <- data.frame(x3, y3)
ggplot(dat3, aes(x=x3, y=y3)) + geom_point() + geom_smooth(method="lm")
```

## Решение, блок 3

```{r fig.show='hold', purl=FALSE}
mod3 <- lm(y3 ~ x3, data = dat3)
op <- par(mfrow = c(1, 3))
plot(mod3, which = 4)
residualPlot(mod3)
qqPlot(mod3)
par(op)
```

\pause

- 100-е наблюдение сильно влияет на ход регрессии
- Зависимость нелинейна

## Что нужно писать в тексте статьи по поводу проверки валидности моделей?

### Вариант 1

Привести необходимые графики в электронных приложениях.

\pause

### Вариант 2

Привести в тексте работы результаты тестов на гомогеность дисперсии, автокорреляцию (если используются пространственые или временные предикторы) и нормальность распределиня остатков.

\pause

### Вариант3

Написать в главе _"Материал и методика"_ фразу вроде такой:   "Визуальная проверка графиков рассяния остатков не выявила заметных отклонений от условий гомогенности дисперсий и нормальности".

## Summary

- Не всякая модель, в которой коэффициенты достоверно отличаются от нуля, может считаться валидной
- Обязательный этап работы с моделями - проверка условий применимости
- Наиболее важную информацию о валидности модели дает анализ остатков

## Что почитать

+ Кабаков Р.И. R в действии. Анализ и визуализация данных на языке R. М.: ДМК Пресс, 2014.
+ Quinn G.P., Keough M.J. (2002) Experimental design and data analysis for biologists, pp. 92-98, 111-130
+ Diez D. M., Barr C. D., Cetinkaya-Rundel M. (2014) Open Intro to Statistics., pp. 354-367.
+ Logan M. (2010) Biostatistical Design and Analysis Using R. A Practical Guide, pp. 170-173, 208-211
+ Legendre P., Legendre L. (2012) Numerical ecology. Second english edition. Elsevier, Amsterdam. 

