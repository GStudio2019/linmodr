---
title: "Смешанные линейные модели"
subtitle: "Линейные модели..."
author: "Марина Варфоломеева, Вадим Хайтов"
output:
  beamer_presentation:
    colortheme: beaver
    highlight: tango
    includes:
      in_header: ./includes/header.tex
    pandoc_args:
    - --latex-engine=xelatex
    - -V fontsize=10pt
    - -V lang=russian
    slide_level: 2
    theme: default
    toc: yes
institute: "СПбГУ"
---

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
# to render
# rmarkdown::render("10_GLMM_gaussian.Rmd", output_format = "beamer_presentation")
# options(width = 70, scipen = 6, digits = 3)
library(knitr)
# chunk default options
opts_chunk$set(fig.show='hold', size='footnotesize', comment="#", warning=FALSE, message=FALSE, dev='cairo_pdf', fig.height=2.5, fig.width=7.7)
# library("extrafont")
source("support_linmodr.R")
```

## Вы узнаете

- Что такое смешаные модели и когда они применяются
- Что такое фиксированные и случайные факторы

### Вы сможете

- Рассказать чем фиксированные факторы отличаются от случайных
- Привести примеры факторов, которые могут быть фиксированными или случайными в зависимости от задачи исследования
- Рассказать, что оценивает коэффициент внутриклассовой корреляции и вычислить его для  случая с одним случайным фактором
- Подобрать смешаную линейную модель со случайным отрезком и случайным углом наклона в R при помощи методов максимального правдоподобия

# "Многоуровневые" данные

## Пример: Как время реакции людей зависит от бессонницы?

В статье  Belenky et al., 2003. приводится такая схема исследования:

\includegraphics[height=0.5\paperheight]{images/Belenky_et_al._2003_fig_1.png}

В датасете `sleepstudy` из пакета `lme4` описание немного отличается от того, что в статье:
В ночь перед нулевым днем всем испытуемым давали поспать нормальное время, а в следующие 9 ночей --- давали спать по 3 часа. Каждый день измеряли время реакции в серии тестов. 

\tiny

Данные: Belenky et al. (2003) Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. Journal of Sleep Research 12, 1–12.


## Данные `sleepstudy`

- `Reaction` --- среднее время реакции в серии тестов в день наблюдения, мс
- `Days` --- число дней депривации сна
- `Subject` --- номер испытуемого

```{r}
library(lme4)
data(sleepstudy)
sl <- sleepstudy
head(sl, 3)
```

## Знакомство с данными

```{r}
str(sl)
# пропущенные значения
colSums(is.na(sl))
```

## Знакомство с данными

```{r}
# число субъектов
length(unique(sl$Subject))
# сбалансирован ли объем выборки?
table(sl$Subject)
table(sl$Subject, sl$Days)
```

## Есть ли выбросы?

```{r}
library(ggplot2)
theme_set(theme_bw())
# построим дот-плот
ggplot(sl, aes(x = Reaction, y = 1:nrow(sl))) +
  geom_point()
```
\pause

Кажется, что нет ничего странного, но мы еще не учли информацию о субъектах

## Как меняется время реакции разных субъектов?

```{r}
ggplot(sl, aes(x = Reaction, y = Subject, colour = Days)) +
  geom_point() 
```

\pause

- Видно, что у разных субъектов время реакции различается. Есть быстрые, есть медленные, кого-то недосып стимулирует. Сама по себе межиндивидуальная изменчивость нас не интересует, но ее нельзя игнорировать.

## Что делать с разными субъектами?

\pause

\begin{minipage}{\linewidth}
\setlength\intextsep{0pt}
\begin{wrapfigure}{l}{0.4\textwidth}
\includegraphics{images/the_good1.jpg}
\end{wrapfigure}
The Good --- подбираем смешанную модель, в которой есть фиксированный фактор `Days` и случайный фактор `Subject`, который опишет межиндивидуальную изменчивость.
\end{minipage}

\vspace{12mm}

\pause

\begin{minipage}{\linewidth}
\setlength\intextsep{0pt}
\begin{wrapfigure}{l}{0.4\textwidth}
\includegraphics{images/the_bad1.jpg}
\end{wrapfigure}
The Bad --- игнорируем структуру данных, подбираем модель с единственным фиксированным фактором `Days`. (Не учитываем группирующий фактор `Subject`). Неправильный вариант.
\end{minipage}

\pause

\vfill

\begin{minipage}{\linewidth}
\setlength\intextsep{0pt}
\begin{wrapfigure}{l}{0.4\textwidth}
\includegraphics{images/the_ugly1.jpg}
\end{wrapfigure}
The Ugly --- подбираем модель с двумя фиксированными факторами: `Days` и `Subject`. (Группирующий фактор `Subject` опишет межиндивидуальную изменчивость как обычный фиксированный фактор).
\end{minipage}

## The Bad. Не учитываем группирующий фактор.

$$Reaction_{i} = \beta_0 + \beta_1 Days_{i} + \varepsilon_{i}$$

$\varepsilon_i \sim N(0, \sigma^2)$  
$i = 1, 2, ..., 180$ -- общее число наблюдений

В матричном виде 
$$\mathbf{Reaction} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}$$


```{r}
Wrong1 <- lm(Reaction ~ Days, data = sl)
```

График этой модели

```{r echo=FALSE}
ggplot(sl, aes(x = Days, y = Reaction)) +
  geom_point() +
  geom_smooth(se = TRUE, method = "lm", size = 1)
```


## The Bad. Не учитываем группирующий фактор.

\small

```{r}
summary(Wrong1)
```

\pause

\normalsize

- Если мы не учитываем группирующий фактор, увеличивается вероятность ошибок I рода. Все будет казаться "очень достоверно" из-за низких стандартных ошибок. Но поскольку в этом случае условие независимости нарушено --- __все не так, как кажется__.

## The Ugly. Группирующий фактор как фиксированный.

$$Reaction_{ij} = \beta_0 + \beta_1 Days_{j} + \beta_{2}Subject_{i = 2} + ... + \beta_{2}Subject_{i = `r length(unique(sl$Subject))`} + \varepsilon_{ij}$$

$\varepsilon_{ij} \sim N(0, \sigma^2)$ - остатки от регрессии  
$i = 1, 2, ..., 18$ - субъект  
$j = 1, 2, ..., 10$ - день

В матричном виде
$$\mathbf{Reaction} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}$$

```{r}
Wrong2 <- lm(Reaction ~ Days + Subject, data = sl)
```

\pause

Если мы учитываем группирующий фактор как обычно (как __фиксированный фактор__), придется оценивать слишком много параметров (`r length(unique(sl$Subject))` для уровней группирующего фактора, 1 для `Days`, $\sigma$ --- всего `r length(coef(Wrong2)) + 1`).
При этом у нас всего `r sum(complete.cases(sl))` наблюдений. Чтобы получить удовлетворительную мощность, нужно минимум 10--20 наблюдений на каждый параметр (Harrell, 2013) --- у нас `r sum(complete.cases(sl))/(length(coef(Wrong2)) + 1)`.

## The Ugly. Что нам делать с этим множеством прямых?

```{r}
Wrong2_diag <- fortify(Wrong2)
ggplot(Wrong2_diag, aes(x = Days, colour = Subject)) +
  geom_line(aes(y = .fitted, group = Subject)) +
  geom_point(data = sl, aes(y = Reaction)) +
  guides(colour = guide_legend(ncol = 2))
```

\pause

В этой модели, где субъект --- это фиксированный фактор, для каждого субъекта есть "поправка" для значения свободного члена в уравнении регрессии. В результате универсальность модели теряется: предсказания можно сделать только на индивидуальном уровне --- с учетом субъекта

# Фиксированные и случайные факторы

## Можно посмотреть на группирующий фактор иначе!

Когда нам не важны конкретные значения интерсептов для разных уровней фактора, мы можем представить, что эффект фактора (величина "поправки") --- случайная величина, и можем оценить дисперсию между уровнями группирующего фактора.

Такие факторы называются __случайными факторами__, а модели с такими факторами называются __смешанными моделями__:

- Общие смешанные модели (general linear mixed models) --- нормальное распределение зависимой переменной

- Обобщенные смешанные модели (generalized linear mixed models) --- другие формы распределений зависимой переменной

## Фиксированные и случайные факторы

\resizebox{1\textwidth}{!}{
\begin{tabular}{L{0.2\textwidth} C{0.4\textwidth} C{0.4\textwidth}}
\hline\noalign{\smallskip}
Свойства & Фиксированные факторы & Случайные факторы \\
\hline\noalign{\smallskip}
Уровни фактора & фиксированные, заранее определенные и потенциально воспроизводимые уровни & случайная выборка из всех возможных уровней \\
Используются для тестирования гипотез & о средних значениях отклика между уровнями фактора \linebreak $H _{0}: \mu _1 = \mu _2 = \ldots = \mu _i = \mu$ & о дисперсии отклика между уровнями фактора \linebreak $H _{0}: \sigma_{rand.fact.}^2 = 0$ \\
Выводы можно экстраполировать & только на уровни из анализа & на все возможные уровни \\
Число уровней фактора & Осторожно! Если уровней фактора слишком много, то нужно подбирать слишком много коэффициентов --- должно быть много данных & Важно! Для точной оценки $\sigma$ нужно нужно много уровней фактора --- не менее 5 \\
\hline\noalign{\smallskip}
\end{tabular}
}

## Примеры фиксированных и случайных факторов

__Фиксированные факторы__

- Пол
- Низина/вершина
- Илистый/песчаный грунт
- Тень/свет
- Опыт/контроль

__Случайные факторы__

- Субъект, особь или площадка (если есть несколько измерений)
- Выводок (птенцы из одного выводка имеют право быть похожими)
- Блок, делянка на участке
- Аквариум в лаб. эксперименте

## Задание 1

Какого типа эти факторы? Поясните ваш выбор.

- Несколько произвольно выбранных градаций плотности моллюсков в полевом эксперименте, где плотностью манипулировали.

- Фактор размер червяка (маленький, средний, большой) в выборке червей.

- Деление губы Чупа на зоны с разной степенью распреснения.

# Cмешанные линейные модели

## Cмешанная линейная модель в общем виде

$$\mathbf{Y}_i = \mathbf{X} _i \cdot \boldsymbol{\beta} + \mathbf{Z}_i \cdot \mathbf{b} _i + \boldsymbol{\varepsilon} _i$$

$\mathbf{b} _i \sim N(0, \mathbf{D})$ --- случайные эффекты нормально распределены со средним 0 и матрицей ковариаций $\mathbf{D}$ (дисперсией $\sigma_{b}^2$)

$\boldsymbol{\varepsilon} _i \sim N(0, \boldsymbol{\Sigma})$ --- остатки модели нормально распределены со средним 0 и матрицей ковариаций $\boldsymbol{\Sigma}_i$ (дисперсией $\sigma^2$)

$\mathbf{X} _i \cdot \boldsymbol{\beta}$ --- фиксированная часть модели

$\mathbf{Z}_i \cdot \mathbf{b} _i$ --- случайная часть модели

## В примере модель со случайным отрезком можно записать так:

$$Reaction_{ij} = \beta_0 + \beta_1 Days_{ij} + b_i + \varepsilon_{ij}$$

$b_{i} \sim N(0, \sigma_b^2)$ --- случайный эффект субъекта (intercept)  
$\varepsilon_{ij} \sim N(0, \sigma^2)$ --- остатки модели  
$i = 1, 2, ..., 18$ --- субъекты  
$j = 1, 2, ..., 10$ --- дни

\pause

Для каждого субъекта $i$ в матричном виде это записывается так:

$$\begin{pmatrix} Reaction _{i1} \\ Reaction _{i2} \\ \vdots \\ Reaction _{i10} \end{pmatrix} 
= \begin{pmatrix}
1 & Days _{i1} \\ 1 & Days _{i2} \\ \vdots \\ 1 & Days _{i10}
\end{pmatrix} 
\cdot
 \begin{pmatrix}
\beta _0 \\ \beta _1
\end{pmatrix}  +
 \begin{pmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{pmatrix} 
\cdot b _{i} +
 \begin{pmatrix} \varepsilon _{i1} \\ \varepsilon _{i2}\\ \vdots \\ \varepsilon _{i10} \end{pmatrix} $$

\pause

что можно записать сокращенно так:

$$\mathbf{Reaction} _i = \mathbf{X} _i \cdot \boldsymbol{\beta} + \mathbf{Z} _i \cdot \mathbf{b} _i + \boldsymbol{\varepsilon}_i$$





## Теперь разберемся с допущениями модели

$$\mathbf{Reaction} _i = \mathbf{X} _i \cdot \boldsymbol{\beta} + \mathbf{Z} _i \cdot \mathbf{b} _i + \boldsymbol{\varepsilon}_i$$

$\mathbf{b} _i \sim N(0, \mathbf{D})$ - случайные эффекты $b _i$ нормально распределены со средним 0 и матрицей ковариаций $\mathbf{D}$  
$\boldsymbol{\varepsilon} _i \sim N(0, \boldsymbol{\Sigma} _i)$ - остатки модели нормально распределены со средним 0 и матрицей ковариаций $\boldsymbol{\Sigma} _i$

\pause

Матрица ковариаций остатков для каждого субъекта выглядит так:
$$\boldsymbol{\Sigma} _i = \sigma^2 \cdot
 \begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{pmatrix} $$

\pause

Т.е. остатки независимы друг от друга (вне диагонали стоят нули, т.е. ковариация разных остатков 0).

В то же время, отдельные значения переменной-отклика $\mathbf{Y} _i$ уже не будут независимы друг от друга при добавлении случайных эффектов - см. ниже

## Матрица ковариаций переменной-отклика

$$\mathbf{Reaction} _i = \mathbf{X} _i \cdot \boldsymbol{\beta} + \mathbf{Z} _i \cdot \mathbf{b} _i + \boldsymbol{\varepsilon}_i$$

$\mathbf{b} _i \sim N(0, \mathbf{D})$  
$\boldsymbol{\varepsilon} _i \sim N(0, \boldsymbol{\Sigma} _i)$


Можно показать, что переменная-отклик $\mathbf{Y} _i$ нормально распределена 

$\mathbf{Y} _i \sim N(\mathbf{X} _i \cdot \boldsymbol{\beta}, \mathbf{V} _i )$

\pause

Матрица ковариаций переменной-отклика:

$$\mathbf{V} _i = \mathbf{Z} _i \mathbf{D} \mathbf{Z'} _i + \boldsymbol{\Sigma} _i$$

где $\mathbf{D}$ --- матрица ковариаций случайных эффектов.

Т.е. __добавление случайных эффектов приводит к изменению ковариационной матрицы__ $\mathbf{V} _i$

\vfill
\footnotesize

Кстати, $\mathbf{Z} _i \mathbf{D} \mathbf{Z'} _i$ называется преобразование Холецкого (Cholesky decomposition)

## Добавление случайных эффектов приводит к изменению ковариационной матрицы

$$\mathbf{V} _i = \mathbf{Z} _i \mathbf{D} \mathbf{Z'} _i + \boldsymbol{\Sigma} _i$$

Для простейшей смешанной модели со случайным отрезком:

$$\mathbf{V} _i =  \begin{pmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{pmatrix}
\cdot \sigma_b^2
\cdot  \begin{pmatrix} 1 & 1 & \cdots & 1 \end{pmatrix} +
\sigma^2
\cdot
 \begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{pmatrix}  =$$

$$
=  \begin{pmatrix}
\sigma^2 + \sigma_b^2 & \sigma_b^2 & \cdots & \sigma_b^2 \\
\sigma_b^2 & \sigma^2 + \sigma_b^2 & \cdots & \sigma_b^2 \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_b^2 & \sigma_b^2 & \sigma_b^2 & \sigma^2 + \sigma_b^2
\end{pmatrix} 
$$

## Индуцированная корреляция - следствие  включения в модель случайных эффектов
$$\mathbf{V} _i =
 \begin{pmatrix}
\sigma^2 + \sigma_b^2 & \sigma_b^2 & \cdots & \sigma_b^2 \\
\sigma_b^2 & \sigma^2 + \sigma_b^2 & \cdots & \sigma_b^2 \\
\vdots & \vdots & \ddots & \vdots \\
\sigma_b^2 & \sigma_b^2 & \sigma_b^2 & \sigma^2 + \sigma_b^2
\end{pmatrix}  
$$

\pause

$\sigma_b^2$ --- ковариация между наблюдениями одного субъекта  
$\sigma^2 + \sigma_b^2$ --- дисперсия

Т.е. корреляция между наблюдениями одного субъекта $\sigma_b^2 / (\sigma^2 + \sigma_b^2)$

\pause

### Коэффициент внутриклассовой корреляции $\sigma_b^2 / (\sigma^2 + \sigma_b^2)$

Способ измерить, насколько коррелируют друг с другом наблюдения из одной и той же группы случайного фактора. Если он высок, то можно брать меньше проб в группе (и больше групп, если нужно)

# Подбор смешанных моделей в R

## Подбор смешанных моделей в R

Самые популярные пакеты --- `nlme` (старый, иногда медленный, стабильный, хорошо документированный) и `lme4` (новый, быстрый, не такой стабильный, хуже документированный). Есть много других.

\resizebox{1\textwidth}{!}{
\begin{tabular}{L{0.2\textwidth} C{0.2\textwidth} C{0.2\textwidth} C{0.2\textwidth} C{0.2\textwidth}}
\hline\noalign{\smallskip}
Функция     & lme() из nlme & lmer() из lme4 & glmer() из lme4 & glmmPQL() из MASS \\
\hline\noalign{\smallskip}
Распределение отклика & нормальное  & нормальное & биномиальное, пуассоновское, гамма, (+ квази) & биномиальное, пуассоновское, гамма, (+ квази), отр. биномиальное \\
Метод оценивания & ML, REML & ML, REML & ML, REML & PQL \\
Гетерогенность дисперсий & + & - & - & - \\
Корреляционные структуры & + & - & - & + \\
Доверительная вероятность (p-value) & + & - & - & + \\
\hline\noalign{\smallskip}
\end{tabular}
}

## Синтаксис для смешанных моделей в R

__Фиксированная часть модели__ задается обычной двухсторонней формулой

`Y ~ 1 + X1 + ... + Xn`

__Случайная часть модели__ - односторонняя формула. До вертикальной черты --- перечислены факторы, влияющие на случайный угол наклона. После вертикальной черты --- факторы, влияющие на случайный intercept.

`~ 1 + X1 + ... + Xn |A`

Вложенные друг в друга факторы указываются от крупного к мелкому через "/"

`~ 1 + X1 + ... + Xn |A/B/C`

Детали синтаксиса разных функций отличаются (см. следующий слайд с примерами формул)

## Синтаксис некоторых смешанных моделей

\small
\resizebox{1\linewidth}{!}{
\begin{tabular}{L{0.35\textwidth} L{0.37\textwidth} L{0.28\textwidth}}
\hline
Факторы & lme() из nlme & lmer() из lme4 \\
\hline
А -- случ. intercept & lme(fixed=Y$\sim$1,random=$\sim$1|A, data=dt) & lmer(Y$\sim$1+(1|A), data=dt) \\
A -- случ. intercept, \linebreak X -- фикс. & lme(fixed=Y$\sim$X,random=$\sim$1|A, data=dt) & lmer(Y$\sim$X+(1|A), data=dt) \\
A -- случ. intercept, \linebreak X -- случ. угол накл. & lme(fixed=Y$\sim$X,random=$\sim$1+X|A, data=dt) & lmer(Y$\sim$X+(1+X|A), data=dt) \\
%A -- случ. intercept, \linebreak X -- фикс. \linebreak A вложен в фикс.Х & nlme(fixed=Y$\sim$X,random=$\sim$1|X/A, data=dt) & lmer(Y$\sim$X+(1|A:X), data=dt) \\
A и В -- случ. intercept, \linebreak  A и B независимы (crossed effects), \linebreak X -- фикс. & & lmer(Y$\sim$X+(1|A)+(1|B), data=dt) \\
A и В -- случ. intercept, \linebreak B вложен в А (nested effects), уровни B повт. в группах по фактору A, \linebreak X -- фикс. & lme(fixed=Y$\sim$X,random=$\sim$1|A/B, data=dt) & lmer(Y$\sim$X+(1|A/B), data=dt) \linebreak lmer(Y$\sim$X+(1|A)+(1|A:B), data=dt) \\
A и В -- случ. intercept, \linebreak B вложен в А (nested random effects), все уровни B уникальны, \linebreak X -- фикс. & lme(fixed=Y$\sim$X,random=$\sim$1|A/B, data=dt) & lmer(Y$\sim$X+(1|A)+(1|B), data=dt) \\
\hline
\end{tabular}
}

# Смешанные модели со случайным отрезком в R

## Подберем модель со случайным отрезком с помощью `lme()` из пакета `nlme`. 

Функция `lme()` из пакета `nlme` нам понадобится на следующем занятии, поэтому нужно освоить ее синтаксис.

```{r}
# выгружаем lme4, чтобы не было конфликтов с nlme
detach(name = "package:lme4") 
library(nlme)
M1 <- lme(Reaction ~ Days, random = ~ 1 | Subject, data = sl)
```

Что дальше?

# Анализ остатков

## График остатков от предсказанных значений

```{r}
M1_diag <- data.frame(sl, 
                      .resid = resid(M1, type = "pearson"), 
                      .fitted <- fitted(M1))
gg_resid <- ggplot(M1_diag, aes(y = .resid)) +
  guides(colour = guide_legend(ncol = 2))
gg_resid + geom_point(aes(x = .fitted, colour = Subject))
```

\pause

- Есть большие остатки, гетерогенность дисперсий

## Графики остатков от ковариат в модели и не в модели

```{r}
library(gridExtra)
grid.arrange(gg_resid + geom_boxplot(aes(x = factor(Days))),
             gg_resid + geom_boxplot(aes(x = Subject)),
             ncol = 2, widths = c(0.4, 0.6))
```

\pause

- Большие остатки у наблюдений для 332 субъекта
- Гетерогенность дисперсий
- Пока оставим все как есть

# Тестирование гипотез в смешанных моделях

## Способы тестирования влияния факторов в смешанных моделях

Достаточно __одного__ из этих равноправных вариантов.

Важно, каким именно способом (ML или REML) подобрана модель.

(а) t-(или -z) тесты  --- приблизительный результат (REML)

(б) F-тест --- приблизительный результат (REML)

(в) Попарное сравнение вложенных моделей при помощи тестов отношения правдоподобий  (ML)

(г) Сравнение моделей по AIC (ML)

## (а) t-(или -z) тесты (REML)

- `summary(model)` 
- Подходит для непрерывных переменных или факторов с 2 уровнями.  
- Дает приблизительный результат, лучше так не делать.

\footnotesize

```{r}
summary(M1)
```

## (б) F-тест (REML)

- `anova()`
- Приблизительный результат, лучше так не делать.
- Последовательное тестирование гипотез (Type I SS) --- будьте внимательны при интерпретации

```{r}
library(car)
anova(M1, test = "F")
```

\pause

- Время реакции зависит от продолжительности бессонницы ($F_{1, 161} = 169$, $p < 0.01$)

## (в) Попарное сравнение вложенных моделей при помощи тестов отношения правдоподобий (ML)

Дает более точные выводы, чем F и t(z)

Обязательно method = "ML", а не "REML"

\footnotesize

```{r}
M1.ml <- lme(Reaction ~ Days, random = ~1|Subject, data = sl, method = "ML")
M2.ml <- lme(Reaction ~ 1, random = ~1 | Subject, data = sl, method = "ML")
```
\footnotesize

Любой из этих вариантов:

- `anova(model1, model2)`
- `drop1()`
- `Anova()` из пакета `car` --- Type II, III SS, не приводится значение отношения правдоподобий

```{r}
anova(M1.ml, M2.ml)
```

\pause

- Время реакции меняется в зависимости от продолжительности бессонницы (L = 116, df = 1, p < 0.01)

## (г) Сравнение моделей по AIC (ML)

Обязательно method = "ML", а не "REML"

```{r}
AIC(M1.ml, M2.ml)
```

\pause

- Продолжительность бессонницы влияет на время реакции (AIC)

# Подбор оптимальной модели и проверка условий применимости

## Подбор оптимальной модели и проверка условий применимости

Если вы решили подбирать оптимальную модель и выкидывать какие-то предикторы, то вам нужно будет сделать анализ остатков финальной модели.

В нашем случае модель не изменилась, поэтому данный этап выпадает из анализа

# Представление результатов

## Представление результатов

REML оценка параметров более точна (оценка случайных факторов)

Для представления результатов лучше использовать модель, подобранную при помощи Restricted Maximum Likelihood.

```{r}
M1_fin <- lme(Reaction ~ Days, random = ~1|Subject, data = sl,
              method = "REML")
```

В данном случае, этот шаг избыточен, т.к. lme использует REML по-умолчанию, и поэтому сейчас нам не нужно было ничего менять.

Но lmer использует ML, и тогда точно нужно переподобрать финальную модель при помощи REML.

## Уравнение модели

$$Reaction_{ij} = `r round(fixef(M1_fin), 1)[1]` + `r round(fixef(M1_fin), 1)[2]` Days_{ij} + b_i + \varepsilon_{ij}$$

$b_{i} \sim N(0, `r round(as.numeric(VarCorr(M1_fin)[2, 2]), 1)`^2)$ --- случайный эффект субъекта  
$\varepsilon_{ij} \sim N(0, `r round(as.numeric(VarCorr(M1_fin)[1, 2]), 1)`^2)$ --- остатки модели  
$i = 1, 2, ..., 18$ --- субъекты  
$j = 1, 2, ..., 10$ --- дни

\vfill
\small

```{r}
fixef(M1_fin)    # Фиксированные эффекты
VarCorr(M1_fin)  # Случайные эффекты
```

## Внутриклассовая корреляция

$\sigma_{effect}^2 / (\sigma_{effect}^2 + \sigma^2)$

```{r}
# Внутриклассовая корреляция
37.12383^2 / (37.12383^2 + 30.99123^2)
```

\small
\vfill

```{r, eval=FALSE}
M1_fin
```

    В результатах
    Random effects:
     Formula: ~1 | Subject
            (Intercept) Residual
    StdDev:    37.12383 30.99123


\pause

- Значения времени реакции одного субъекта похожи. Высокая внутриклассовая корреляция показывает, что эффект субъекта нельзя игнорировать в анализе.


## Данные для графика предсказаний фиксированной части модели

```{r}
# Исходные данные
library(plyr)
NewData_M1 <- ddply(
  sl, .(Subject), summarise,
  Days = seq(min(Days), max(Days), length = 10)
  )

# Предсказанные значения при помощи predict()
# level = 0 - для фиксированных эффектов (т.е. без учета субъекта)
NewData_M1$fitted <- predict(M1_fin, NewData_M1, level = 0)

# Предсказанные значения при помощи матриц
X <- model.matrix(~ Days, data = NewData_M1)
betas <- fixef(M1_fin)
NewData_M1$fitted <- X %*% betas

# Cтандартные ошибки и дов. интервалы
NewData_M1$se <- sqrt( diag(X %*% vcov(M1_fin) %*% t(X)) )
NewData_M1$lwr <- NewData_M1$fitted - 1.98 * NewData_M1$se
NewData_M1$upr <- NewData_M1$fitted + 1.98 * NewData_M1$se
```

## График предсказаний фиксированной части модели

```{r}
ggplot(data = NewData_M1, aes(x = Days, y = fitted)) +
  geom_ribbon(alpha = 0.35, aes(ymin = lwr, ymax = upr)) +
  geom_line() +
  geom_point(data = sl, aes(x = Days, y = Reaction))
```

## Данные для графика предсказаний для индивидуальных уровней случайного фактора

Если вам любопытно, куда делась информация о разных субъектах, то вот она...

Можно получить предсказания для каждого субъекта

$\beta_0 + \beta_1 \cdot Days_{ij} + b_i$

```{r}
NewData_M1$fit_subj <- predict(M1_fin, NewData_M1, level = 1)
# или то же самое при помощи матриц
# случайные эффекты для каждого субъекта
# это датафрейм с одним столбцом
rand <- ranef(M1_fin)
# "разворачиваем" для каждой строки данных
all_rand <- rand[as.numeric(NewData_M1$Subject), 1]
# прибавляем случайные эффекты к предсказаниям фикс. части
NewData_M1$fit_subj <- X %*% betas + all_rand
```


## График предсказаний для индивидуальных уровней случайного фактора


```{r gg_M1_subj}
ggplot(NewData_M1, aes(x = Days, y = fit_subj, group = Subject)) +
  geom_ribbon(alpha = 0.5, aes(fill = Subject, ymin = fit_subj - 1.98*se, 
                  ymax = fit_subj + 1.98*se)) + 
  geom_line() +
  geom_point(data = sl, aes(x = Days, y = Reaction))  +
  guides(fill = guide_legend(ncol = 2))
```
\pause

Не факт, что на самом деле время реакции разных субъектов меняется параллельно

# Смешанные модели со случайным отрезком и углом наклона

## Смешанная модель со случайным отрезком и углом наклона

На графике индивидуальных эффектов было видно, что измерения для разных субъектов, возможно, идут непараллельными линиями. Усложним модель --- добавим случайные изменения угла наклона для каждого из субъектов.

Это можно биологически объяснить. Возможно, в зависимости от продолжительности бессонницы у разных субъектов скорость реакции будет ухудшаться разной скоростью: одни способны выдержать 9 дней почти без потерь, а другим уже пары дней может быть достаточно.

## Уравнение модели со случайным отрезком и углом наклона

$$Reaction_{ij} = \beta_0 + \beta_1 Days_{ij} + b_i + c_{ij} Days_{ij} + \varepsilon_{ij}$$
  
$b_{i} \sim N(0, \sigma_b^2)$ --- случайный интерсепт для субъекта  
$c_{ij} \sim N(0, \sigma_c^2)$ --- случайный угол наклона для субъекта  
$\varepsilon_{ij} \sim N(0, \sigma^2)$ --- остатки модели  
$i = 1, 2, ..., 18$ --- субъекты  
$j = 1, 2, ..., 10$ --- дни


## Дальнейшие действия по прежнему плану:

- Подбираем модель
- Анализ остатков
- Проверка влияния факторов + подбор оптимальной модели
- Анализ остатков финальной модели
- Подбор финальной модели при помощи REML
- Описание результатов
- Визуализация предсказаний

## Смешанная модель со случайным отрезком и углом наклона в R

Формат записи формулы для случайных эффектов в `lme()`:

    random = ~ 1 + Угол наклона | Интерсепт


```{r}
MS1 <- lme(Reaction ~ Days, random = ~ 1 + Days|Subject, data = sl)
```

## Задание 2

Проверьте получившуюся модель MS1

Сделайте самостоятельно:

- Анализ остатков
- Проверку влияния факторов + подбор оптимальной модели
- Визуализацию предсказаний

## Решение: График остатков от предсказанных значений

```{r purl=FALSE}
MS1_diag <- data.frame(sl, 
                      .resid = resid(MS1, type = "pearson"), 
                      .fitted <- fitted(MS1))
gg_resid_1 <- ggplot(MS1_diag, aes(y = .resid)) +
  guides(colour = guide_legend(ncol = 2))
gg_resid_1 + geom_point(aes(x = .fitted, colour = Subject))
```

\pause

- Есть большие остатки, гетерогенность дисперсий не выражена

## Решение: Графики остатков от ковариат в модели и не в модели

```{r purl=FALSE}
grid.arrange(gg_resid_1 + geom_boxplot(aes(x = factor(Days))),
             gg_resid_1 + geom_boxplot(aes(x = Subject)),
             ncol = 2, widths = c(0.4, 0.6))
```

\pause

- Большие остатки у наблюдений 332 субъекта
- Гетерогенность дисперсий уже не так сильно выражена, как в прошлый раз.

## Решение: Проверка влияния факторов

Тестируем значимость влияния продолжительности бессонницы. Сделаем это при помощи теста отношения правдоподобий.

```{r purl=FALSE}
MS1.ml <- lme(Reaction ~ Days, random = ~1 + Days|Subject, data = sl, 
              method = "ML")
MS2.ml <- update(MS1.ml, . ~ . - Days)
anova(MS1.ml, MS2.ml)
```
\pause

- Время реакции меняется в зависимости от продолжительности бессонницы (L = 24, df = 1, p < 0.01). 

## Решение: Проверка влияния факторов (случайный интерсепт для субъектов)

Почему мы не тестируем значимость самого фактора Subject? 

Потому что этот фактор у нас должен быть в модели по-определению, без обсуждения --- из-за того, что у нас такой дизайн эксперимента.

## Решение: Проверка влияния факторов (случайный угол наклона для субъектов)

Можем проверить, значимы ли изменения угла наклона для разных субъектов.

__Это случайный фактор --- используем REML__

```{r purl=FALSE}
MS1.reml <- lme(Reaction ~ Days, random = ~1 + Days|Subject, data = sl, 
                method = "REML")
MS3.reml <- update(MS1.reml, random = ~1|Subject)
anova(MS1.reml, MS3.reml)
```

\pause

- Скорость изменений зависит от субъекта (L = 43, df = 2, p < 0.01)

## Решение: Представление результатов

Для представления результатов переподбираем модель заново, используя Restricted Maximum Likelihood.

REML оценка параметров более точна (оценка случайных факторов)

```{r purl=FALSE}
MS1_fin <- lme(Reaction ~ Days, random = ~1 + Days|Subject, data = sl,
               method = "REML")
```

Здесь это избыточный шаг, у нас уже есть такая модель --- MS1.reml

## Решение: Уравнение модели

$$Reaction_{ij} = `r round(fixef(MS1_fin), 1)[1]` + `r round(fixef(MS1_fin), 1)[2]` Days_{ij} + b_i + c_{ij} Days_{ij} + \varepsilon_{ij}$$
  
$b_{i} \sim N(0, `r round(as.numeric(VarCorr(MS1_fin)[1, 2]), 1)`^2)$ --- случайный интерсепт для субъекта  
$c_{ij} \sim N(0, `r round(as.numeric(VarCorr(MS1_fin)[2, 2]), 1)`^2)$ --- случайный угол наклона для субъекта  
$\varepsilon_{ij} \sim N(0, `r round(as.numeric(VarCorr(MS1_fin)[3, 2]), 1)`^2)$ --- остатки модели  
$i = 1, 2, ..., 18$ --- субъекты  
$j = 1, 2, ..., 10$ --- дни

\vfill
\small

```{r  purl=FALSE}
fixef(MS1_fin)    # Фиксированные эффекты
VarCorr(MS1_fin)  # Случайные эффекты
```

<!-- Внутриклассовая корреляция для моделей со случайным углом наклона --- Goldstein et al. (2002). Не очень полезна, т.к. не может быть интерпретирована просто как доля изменчивости. -->


## Решение: Данные для графика предсказаний фиксированной части модели

```{r purl=FALSE}
# Исходные данные
NewData_MS1 <- ddply(
  sl, .(Subject), summarise,
  Days = seq(min(Days), max(Days), length = 10)
  )

# Предсказанные значения при помощи predict()
# level = 0 - для фиксированных эффектов (т.е. без учета субъекта)
NewData_MS1$fitted <- predict(MS1_fin, NewData_MS1, level = 0)

# Предсказанные значения при помощи матриц
X <- model.matrix(~ Days, data = NewData_MS1)
betas = fixef(MS1_fin)
NewData_MS1$fit <- X %*% betas

# Cтандартные ошибки и дов. интервалы
NewData_MS1$se <- sqrt( diag(X %*% vcov(MS1_fin) %*% t(X)) )
NewData_MS1$lwr <- NewData_MS1$fit - 1.98 * NewData_MS1$se
NewData_MS1$upr <- NewData_MS1$fit + 1.98 * NewData_MS1$se
```

## Решение: График предсказаний фиксированной части модели

```{r purl=FALSE}
ggplot(data = NewData_MS1, aes(x = Days, y = fitted)) +
  geom_ribbon(alpha = 0.35, aes(ymin = lwr, ymax = upr)) +
  geom_line() +
  geom_point(data = sl, aes(x = Days, y = Reaction))
```

## Решение: Данные для графика предсказаний для индивидуальных уровней случайного фактора

Если вам любопытно, куда делась информация о разных субъектах, то вот она...

Можно получить предсказания для каждого субъекта

$\beta_0 + \beta_1 \cdot Days_{ij} + b_i + c_{ij} \cdot Days_{ij}$

```{r purl=FALSE}
NewData_MS1$fit_subj <- predict(MS1_fin, NewData_MS1, level = 1)
# или то же самое при помощи матриц
# случайные эффекты для каждого субъекта
# это датафрейм с двумя столбцами
rand <- ranef(MS1_fin)
# "разворачиваем" для каждой строки данных
all_rand <- rand[as.numeric(NewData_MS1$Subject), ]
# прибавляем случайные эффекты к предсказаниям фикс. части
NewData_MS1$fit_subj <- (betas[1] + all_rand[, 1]) + (betas[2] + all_rand[, 2]) * NewData_MS1$Days
```


## Решение: График предсказаний для индивидуальных уровней случайного фактора

```{r purl=FALSE}
ggplot(NewData_MS1, aes(x = Days, y = fit_subj, group = Subject)) +
  geom_ribbon(alpha = 0.5, aes(fill = Subject, ymin = fit_subj - 1.98*se,
                  ymax = fit_subj + 1.98*se)) +
  geom_line() +
  geom_point(data = sl, aes(x = Days, y = Reaction)) +
  guides(fill = guide_legend(ncol = 2))
```

# Смешанные модели со вложенными случайными факторами


## Вложенные факторы (Nested effects)

### Факторы образуют иерархическую последовательность вложенности

- лес -->  дерево в лесу --> ветка на дереве --> наблюдение (личинки насекомых)

### Внутри каждого уровня главного фактора будут разные (нестрого сопоставимые) уровни вложенного фактора

Деревья, с которых собирали личинок, будут разные в разных лесах (разные экземпляры).  

### Уровни вложенных факторов описывают иерархию взаимного сходства наблюдений

Личинки с разных деревьев из одного леса имеют право быть похожими друг на друга больше, чем на личинок из другого леса  
Личинки на одном дереве имеют право быть похожими друг на друга больше, чем на личинок с другого дерева  
И т.п.

## Другие примеры вложенных факторов

- регион --> город --> больница --> наблюдение (пациент)

- самка --> выводок --> наблюдение (особь)

- лес --> дерево в лесу --> гнездо на дереве --> наблюдение (птенец)

- улитка --> спороциста в улитке --> наблюдение (редия)


## Пример: Высота растений и выпас скота

Вообще-то, статья Gennet et al. 2017 о птицах, но чтобы про них что-то лучше понять, нужно разобраться с их местообитанием.

Как в разные годы высота растительного покрова зависит от выпаса скота, экспозиции склона и проективного покрытия местных растений?

\small

Зависимая переменная:

- **height** - высота растительного покрова

Фиксированные предикторы:

- **graze** - выпас коров (0, 1)
- **AspectCat** - экспозиция (S, N)
- **nativecov** - покрытие местной флоры %
- **slope** - наклон
- **year** - год наблюдений

Случайные предикторы:

- **Park** - парк
- **plotID** - уникальный идентификатор участка

\tiny

Данные:
Gennet, S., Spotswood, E., Hammond, M. and Bartolome, J.W., 2017. Livestock grazing supports native plants and songbirds in a California annual grassland. PloS one, 12(6), p.e0176367.

## Открываем данные

Откроем и переформатируем данные так, чтобы не было дублирования и каждому участку соответствовала одна строчка.

```{r}
library(readxl)
library(tidyr)
gr <- read_excel("data/Grazing_native_plants_Gennet_et_al._2017_S1.xlsx")
graz <- gr %>% spread(Species, presence)
```

## Знакомство с данными

Есть ли пропущенные значения?

```{r}
sum(is.na(graz))
```

Сколько участков было в каждом парке в каждый год?

```{r}
with(graz, table(Park, year))
```

## Наводим порядок

Сделаем факторами переменные, которые понадобятся для модели

```{r}
graz$graze_f <- factor(graz$graze)
graz$AspectCat <- factor(graz$AspectCat)
graz$year_f <- factor(graz$year)
```

Извлечем корень из обилия местных видов

```{r}
graz$nativecov_sq <- sqrt(graz$nativecov) 
```

## Модель

Вспомним главный вопрос исследования и подберем модель

Как в разные годы высота растительного покрова зависит от выпаса скота, экспозиции склона и проективного покрытия местных растений?

Нам нужно учесть, что в разные годы из-за кучи разных причин высота растений может различаться

Кроме того, нужно учесть, что в разных парках и на разных участках растения будут расти сходным образом в разные годы. У нас есть иерархические факторы парк и участок в парке

```{r}
MN1 <- lme(height ~ graze_f*AspectCat + year_f + nativecov_sq + slope,
          random = ~ 1|Park/plotID, 
          data = graz, method = "ML")
```

## Анализ остатков

```{r}
# Данные для анализа остатков
MN1_diag <- data.frame(
  graz,
  pear_res = residuals(MN1, type = "pearson"),
  fitted = fitted(MN1, type = "response"))
```

## График остатков

```{r}
gg_res <- ggplot(data = MN1_diag, aes(y = pear_res))
gg_res + geom_point(aes(x = fitted)) +
  geom_smooth(aes(x = fitted))
```

## Графики остатков от переменных в модели

```{r}
library(gridExtra)
grid.arrange(gg_res + geom_boxplot(aes(x = graze_f)),
gg_res + geom_boxplot(aes(x = AspectCat)),
gg_res + geom_boxplot(aes(x = year_f)),
gg_res + geom_point(aes(x = nativecov_sq)),
gg_res + geom_point(aes(x = slope)),
ncol = 3)
```

>- Паттерн на графике `nativecov_sq`. Возможно, здесь нужно использовать GAMM.

## Графики остатков от переменных не в модели

```{r}
grid.arrange(
  gg_res + geom_point(aes(x = heatloadrel)),
  gg_res + geom_point(aes(x = sqrt(litt))),
  gg_res + geom_point(aes(x = sqrt(bare))),
  ncol = 3)
```
- Паттерн на графике `heatloadrel`

## Результаты полной модели

\tiny

```{r}
summary(MN1)
```

## Тесты отношения правдоподобий для полной модели

```{r}
library(car)
Anova(MN1)
```

Высота растительного покрова:

- на склонах разной экспозиции по-разному зависит от выпаса скота (достоверное взаимодействие)
- различается в разные годы
- не зависит от покрытия местных растений и крутизны склона

## Задание 3

Рассчитайте внутриклассовую корреляцию

- Для наблюдений на одном и том же участке
- Для наблюдений в одном и том же парке

## Внутриклассовая корреляция

$\sigma_{effect}^2 / (\sigma_{effect}^2 + \sigma^2)$

```{r purl=FALSE}
# Для наблюдений на одном и том же участке
3.087577^2 / (1.022106^2 + 3.087577^2 + 5.053857^2)
# Для наблюдений в одном и том же парке
1.022106^2 / (1.022106^2 + 3.087577^2 + 5.053857^2)
```

\small
\vfill

```{r, eval=FALSE, purl=FALSE}
MN1
```

В результатах
```
Random effects:
 Formula: ~1 | Park
        (Intercept)
StdDev:    1.022106
 Formula: ~1 | plotID %in% Park
        (Intercept) Residual
StdDev:    3.087577 5.053857
```

- Значения высоты травяного покрова похожи внутри участка. Сходство наблюдений с разных участков одного парка слабее.

## Данные для графика предсказаний фиксированной части модели

```{r}
# Исходные данные
NewData_MN1 <- expand.grid(graze_f = levels(graz$graze_f),
            AspectCat = levels(graz$AspectCat),
            year_f = levels(graz$year_f))
NewData_MN1$nativecov_sq <- mean(graz$nativecov_sq)
NewData_MN1$slope <- mean(graz$slope)

# Предсказанные значения при помощи матриц
X <- model.matrix(~ graze_f * AspectCat + year_f + nativecov_sq + slope, data = NewData_MN1)
betas = fixef(MN1)
NewData_MN1$fitted <- X %*% betas

# Cтандартные ошибки и дов. интервалы
NewData_MN1$se <- sqrt( diag(X %*% vcov(MN1) %*% t(X)) )
NewData_MN1$lwr <- NewData_MN1$fit - 1.98 * NewData_MN1$se
NewData_MN1$upr <- NewData_MN1$fit + 1.98 * NewData_MN1$se
```

## График предсказаний фиксированной части модели

На южных склонах высота травы выше там, где не пасут скот, а на северных нет. (Строго говоря, нужен еще пост хок тест, чтобы это утверждать.)

```{r}
ggplot(data = NewData_MN1, aes(x = year_f, y = fitted, colour = graze_f)) +
  geom_pointrange(aes(ymin = lwr, ymax = upr)) +
  facet_wrap(~ AspectCat) +
  geom_jitter(data = graz, aes(y = height), alpha = 0.35, size = 1) +
  theme(axis.text.x = element_text(angle = 90))
```

# Вариант решения с подбором оптимальной модели (самостоятельно)

## Подбор оптимальной модели (1)

```{r purl=FALSE}
drop1(MN1, test = "Chi")
```


## Подбор оптимальной модели (2)

```{r purl=FALSE}
MN1.1 <- update(MN1, .~.-slope)
drop1(MN1.1, test = "Chi")
```


## Подбор оптимальной модели (3)

```{r purl=FALSE}
MN1.2 <- update(MN1.1, .~.-nativecov_sq)
drop1(MN1.2, test = "Chi")
```

## Анализ остатков

```{r purl=FALSE}
# Данные для анализа остатков
MN1.2_diag <- data.frame(
  graz,
  pear_res = residuals(MN1.2, type = "pearson"),
  fitted = fitted(MN1.2, type = "response"))
```

## График остатков

```{r purl=FALSE}
gg_res <- ggplot(data = MN1.2_diag, aes(y = pear_res))
gg_res + geom_point(aes(x = fitted)) +
  geom_smooth(aes(x = fitted))
```

## Графики остатков от переменных в модели

```{r purl=FALSE}
 library(gridExtra)
grid.arrange(gg_res + geom_boxplot(aes(x = graze_f)),
gg_res + geom_boxplot(aes(x = AspectCat)),
gg_res + geom_boxplot(aes(x = year_f)),
ncol = 3)
```


## Графики остатков от переменных не в модели

```{r purl=FALSE}
grid.arrange(
  gg_res + geom_point(aes(x = heatloadrel)),
  gg_res + geom_point(aes(x = sqrt(litt))),
  gg_res + geom_point(aes(x = sqrt(bare))),
  gg_res + geom_point(aes(x = nativecov_sq)),
  gg_res + geom_point(aes(x = slope)),
  ncol = 3)
```
- Паттерн на графике `heatloadrel`, `nativecov_sq`

## Результаты после оптимизации

\tiny

```{r purl=FALSE}
summary(MN1.2)
```

## Тесты отношения правдоподобий

```{r purl=FALSE}
Anova(MN1.2)
```

Высота растительного покрова:

- на склонах разной экспозиции по-разному зависит от выпаса скота (достоверное взаимодействие)
- различается в разные годы
- не зависит от покрытия местных растений и крутизны склона




## Take-home messages

- Смешанные модели могут включать случайные и фиксированные факторы.
    - Градации фиксированных факторов заранее определены, а выводы можно экстраполировать только на такие уровни, которые были задействованы в анализе. Тестируется гипотеза о равенстве средних в группах.
    - Градации случайных факторов --- выборка из возможных уровней, а выводы можно экстраполировать на другие уровни. Тестируется гипотеза о дисперсии между группами.
- Коэффициент внутриклассовой корреляции оценивает, насколько коррелируют друг с другом наблюдения из одной и той же группы случайного фактора.
- Случайные факторы могут описывать вариацию как интерсептов, так и коэффициентов угла наклона.
- Модели со смешанными эффектами позволяют получить предсказания как общем уровне, так и на уровне отдельных субъектов.

## Дополнительные ресурсы

- Crawley, M.J. (2007). The R Book (Wiley).
- Zuur, A. F., Hilbe, J., & Ieno, E. N. (2013). A Beginner's Guide to GLM and GLMM with R: A Frequentist and Bayesian Perspective for Ecologists. Highland Statistics.
- Zuur, A.F., Ieno, E.N., Walker, N., Saveliev, A.A., and Smith, G.M. (2009). Mixed Effects Models and Extensions in Ecology With R (Springer).


