---
title: "Сравнение линейных моделей"
author: Марина Варфоломеева, Вадим Хайтов
output:
  ioslides_presentation:
    widescreen: true
    css: assets/my_styles.css
    logo: assets/Linmod_logo.png
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
# output options
options(width = 70, scipen = 6)
library(knitr)
# chunk default options
opts_chunk$set(fig.align='center', tidy = FALSE, fig.width = 7, fig.height = 3, warning = FALSE)
```

## Мы рассмотрим

- Принципы выбора лучшей линейной модели
- Переобучение моделей
- Метод максимального правдоподобия
- Сравнение линейных моделей разными способами:
    - частный F-критерий
    - тест отношения правдоподобий
    - информационный критерий Акаике (AIC)

### Вы сможете

- Объяснить связь между качеством описания существующих данных и краткостью модели
- Объяснить, что такое "переобучение" модели
- Определять, какие модели являются вложенными
- Объяснить, как подбирают параметры линейных моделей методом максимального правдоподобия
- Сравнивать вложенные модели при помощи частного F-критерия или теста отношения правдоподобий
- Сравнивать модели при помощи AIC


```{r, echo=FALSE, message=FALSE, purl=FALSE}
library(ggplot2)
theme_set(theme_bw())
library(gridExtra)
```

# Принципы выбора лучшей линейной модели

"Essentially, all models are wrong,  
but some are useful"  
Georg E. P. Box


## Важно не только тестирование гипотез, но и построение моделей

- Проверка соответствия наблюдаемых данных предполагаемой функциональной связи между зависимой перменной и предикторами:
    - оценки параметров,
    - __тестирование гипотез__,
    - оценка объясненной изменчивости ($R^2$),
    - анализ остатков 

- __Построение моделей__ для предсказания значений в новых условиях:
    - Выбор оптимальной модели
    - Оценка предсказательной способности модели

```{r echo=FALSE, purl=FALSE}
library(tidyr)
library(dplyr)

n <- 30
max_span <- 0.9
my_spans <- seq(0.2, max_span, by = 0.1)
sd_e <- 1.5

# Training set -----------------------------------
set.seed(329323)
data_training <- data.frame(x = runif(n, 1, 10)) %>%
  mutate(y = 2*sin(x) + 1 * x  + rnorm(n, 0, 0.9)) %>%
  crossing(span = my_spans) %>%
  group_by(span) %>%
  nest(x, y, .key = "data")

# Trained models
mod_training <- data_training %>%
  mutate(model = purrr::map2(data, span, ~ loess(y ~ x, data = .x, span = .y)))

# Predictions on the training set
pred_training <- mod_training %>%
  mutate(pred = purrr::map2(model, data, ~ predict(.x, newdata = .y))) %>%
  unnest(data, pred, .id = "span") %>%
  mutate(span = factor(span, labels = my_spans))
# MSE training
MSe_training <- pred_training %>%
  group_by(span) %>%
  summarise(training = mean(y - pred)^2)

# Testing set ------------------------------------
set.seed(75)
data_testing <- data.frame(x = runif(n, 1, 10)) %>%
  mutate(y = 2 * sin(x) + 1 * x  + rnorm(n, 0, sd_e)) %>%
  crossing(span = my_spans) %>%
  group_by(span) %>%
  nest(x, y, .key = "data")
# The same model
data_testing$model <- mod_training$model
# Predictions on the testing set
pred_testing <- data_testing %>%
  mutate(pred = purrr::map2(model, data, ~ predict(.x, newdata = .y))) %>%
  unnest(data, pred, .id = "span") %>%
  mutate(span = factor(span, labels = my_spans))
  # MSE testing
MSe_testing <- pred_testing %>% group_by(span) %>%
  summarise(testing = mean(y - pred, na.rm = TRUE)^2)

# All predictions of the smoothers ---------------
data_all <- data.frame(x = seq(1, 10, length.out = 400)) %>%
  crossing(span = my_spans) %>%
  group_by(span) %>%
  nest(x, .key = "data")
# The same model
data_all$model <- mod_training$model
# Predictions on an artificial grid for plotting
pred_all <- data_all %>%
  mutate(pred = purrr::map2(model, data, ~ predict(.x, newdata = .y))) %>%
  unnest(data, pred, .id = "span") %>%
  mutate(span = factor(span, labels = my_spans))

# Plot of predictions -----------------------
gg_dat <- ggplot(pred_all, aes(x = x, y = pred, colour = span, group = span)) +
  geom_point(data = pred_training, aes(x = x, y = y), colour = "black", shape = 19) +
  # geom_point(data = pred_testing, aes(x = x, y = y), colour = "black", shape = 21) +
  # geom_line(size = 1) +
  # facet_wrap(~as.factor(span), nrow = 2) +
  labs(colour = "Сложность\nмодели", y = "y") +
  theme(legend.position = "bottom")

# Plot of predictions with training set -------
gg_train <- ggplot(pred_all, aes(x = x, y = pred, colour = span, group = span)) +
  geom_point(data = pred_training, aes(x = x, y = y), colour = "black", shape = 19) +
  # geom_point(data = pred_testing, aes(x = x, y = y), colour = "black", shape = 21) +
  geom_line(size = 1) +
  facet_wrap(~as.factor(span), nrow = 2) +
  labs(colour = "Сложность\nмодели", y = "y") +
  theme(legend.position = "bottom")


# Plot of predictions with training and testing sets -------
gg_pred <- ggplot(pred_all, aes(x = x, y = pred, colour = span, group = span)) +
  geom_point(data = pred_training, aes(x = x, y = y), colour = "black") +
  geom_point(data = pred_testing, aes(x = x, y = y), colour = "black", shape = 21) +
  geom_line(size = 1) +
  facet_wrap(~as.factor(span), nrow = 2) +
  labs(colour = "Сложность\nмодели", y = "y") +
  theme(legend.position = "bottom")

# Plot of training and testing MSe -----
dat_mse <- merge(MSe_training, MSe_testing) %>% gather(Data, MSe, -span) %>%
  mutate(Complexity = factor(span,
                             levels = sort(my_spans, decreasing = TRUE),
                             labels = sort(my_spans, decreasing = TRUE)),
         Data = factor(Data,
                levels = c("training", "testing"), 
                labels = c("испольованные в модели",
                           "новые")))
gg_mse <- ggplot(dat_mse, aes(x = Complexity, y = MSe, group = Data)) +
  geom_line() +
  geom_point(aes(shape = Data), size = 3) +
  scale_shape_manual(values = c(21, 19), guide = guide_legend(nrow = 2)) +
  labs(x = "Сложность модели", y = "Ошибка предсказаний", shape = "Данные") +
  theme(legend.position = "bottom")
```

## Зачем может понадобится упрощать модель?

### Какую модель можно подобрать для описания этой закономерности?


```{r, echo=FALSE, fig.height=4, fig.width=3, purl=FALSE}
gg_dat
```

>- Для этих данных можно подобрать несколько моделей

## Какая из этих моделей лучше описывает данные? {.smaller}

```{r models-no-labs, echo=FALSE, fig.height=4, fig.width=10, purl=FALSE}
gg_train
```

- Некоторые модели недообучены (underfitted) --- слишком мало параметров, предсказания неточны.   
- Другие модели переобучены (overfitted) --- слишком много параметров, предсказывают еще и случайный шум.

## Что будет, если оценить точность предсказания моделей на новых данных? {.smaller}

```{r echo=FALSE, purl=FALSE, fig.width=10, fig.height=4.5}
gg_pred
```

На новых данных предсказания моделей не будут идеальными.


## Что происходит точностью предсказаний на исходных и новых данных при усложнении модели? {.smaller}

```{r echo=FALSE, purl=FALSE, fig.width=10, fig.height=3.5}
grid.arrange(gg_pred, gg_mse, nrow = 1, widths = c(0.55, 0.45))
```

Ошибка предсказаний на новых данных практически всегда больше, чем на исходных данных. Более сложные модели лучше описывают существующие данные, но на новых данных их предсказания хуже.

При усложнении модели:

- ошибки предсказаний на исходных данных убывают (иногда, до какого-то уровня) (L-образная кривая)
- ошибки предсказаний на новых данных убывают, затем возрастают из-за переобучения (U-образная кривая) 

## Аккуратность и точность предсказаний на новых данных

```{r echo=FALSE, purl=FALSE, fig.height=4}
library(ggforce)

n_reps <- 6
set.seed(9328)
dfr <- data.frame(
  x0 = rep(0, 6),
  y0 =  rep(0, 6),
  r = seq(1, 0.1, length.out = 6),
  fl = rep(1:2, 3)
)
dart <- ggplot() + geom_circle(data = dfr, aes(x0 = x0, y0 = y0, r = r, fill = fl)) +
  scale_fill_gradient(low = "#9ecae1", high = "#deebf7", guide = "none") +
  coord_equal() + theme_void() + theme(plot.title = element_text(hjust = 0.5))

HvLb <- dart +
  annotate(geom = "point", size = 3, shape = 21, colour = "black", fill = "orange",
                x = rnorm(n_reps, 0, 0.25), y = rnorm(n_reps, 0, 0.25)) +
  labs(title = "Большая дисперсия, \nмаленькая погрешность")

LvHb <- dart +
  annotate(geom = "point", size = 3, shape = 21, colour = "black", fill = "orange",
                x = rnorm(n_reps, 0, 0.1), y = rnorm(n_reps, 0, 0.1)) +
labs(title = "Маленькая дисперсия, \nмаленькая погрешность")

HvHb <- dart +
  annotate(geom = "point", size = 3, shape = 21, colour = "black", fill = "orange",
                x = rnorm(n_reps, 0.4, 0.25), y = rnorm(n_reps, 0.3, 0.25)) +
labs(title = "Большая дисперсия, \nбольшая погрешность")

LvLb <- dart +
  annotate(geom = "point", size = 3, shape = 21, colour = "black", fill = "orange",
                x = rnorm(n_reps, 0.4, 0.1), y = rnorm(n_reps, 0.3, 0.1)) +
  labs(title = "Маленькая дисперсия, \nбольшая погрешность")

grid.arrange(HvLb, HvHb, LvHb, LvLb, nrow = 2)
```

Источники ошибок для предсказаний:

$$ошибка = дисперсия + (погрешность)^2 + случайная~неснижаемая~ошибка$$

## Компромисс между погрешностью и разбросом значений предсказаний (Bias-Variance Tradeoff)

```{r echo=FALSE, purl=FALSE}
ggplot(data = data.frame(x = seq(-2, 2, by = 0.1)), aes(x = x)) +
  stat_function(fun = function(x) (0.5)^x, 
                size = 2, aes(colour = "Погрешность2")) +
  stat_function(fun = function(x) 2^x, 
                size = 2, aes(colour = "Дисперсия")) +
  stat_function(fun = function(x) 1.5, 
                size = 2, aes(colour = "Случайная дисперсия")) +
  stat_function(fun = function(x) (0.5)^x + 2^x + 1.5, 
                size = 2, aes(colour = "Полная ошибка")) +
  scale_colour_manual(values = c("Погрешность2" = "darkcyan",
                                 "Дисперсия" = "dodgerblue",
                                 "Случайная дисперсия" = "pink3",
                                 "Полная ошибка" = "orangered3")) +
  theme_classic() + theme(axis.text = element_blank())  +
  labs(x = "Сложность модели", y = "Ошибка", colour = "Источник ошибок")
```

$$ошибка = дисперсия + (погрешность)^2 + случайная~неснижаемая~ошибка$$

## Критерии и методы выбора моделей зависят от задачи

### Объяснение закономерностей

- Нужны точные тесты влияния предикторов: F-тесты или тесты отношения правдоподобий (likelihood-ratio tests)

### Описание функциональной зависимости

- Нужна точность оценки параметров

### Предсказание значений зависимой переменной

- Парсимония: "информационные" критерии (АIC, BIC, и т.д.)
- Нужна оценка качества модели на данных, которые не использовались для ее первоначальной подгонки: методы ресамплинга (кросс-валидация, бутстреп)

### Не позволяйте компьютеру думать за вас!

- Хорошая модель должна соответствовать условиям применимости, иначе вы не сможете доверять результатам тестов.

- Другие соображения: разумность, целесообразность модели, простота, ценность выводов, важность предикторов.

# Знакомство с данными для множественной линейной регрессии

## Пример: птицы в лесах Австралии

От каких характеристик лесного участка зависит обилие птиц в лесах юго-западной Виктории, Австралия (Loyn, 1987)

Переменных много, мы хотим из них выбрать __оптимальный небольшой__ набор.


<div class="columns-2">

![forest in Victoria, Australia](images/vict_m.jpg)
Mystic Forest - Warburton, Victoria by ¡kuba! on flickr



56 лесных участков:

- ABUND - обилие птиц
- AREA - площадь участка
- YRISOL - год изоляции участка
- DIST - расстояние до ближайшего леса
- LDIST - расстояние до ближайшего большого леса
- GRAZE - пастбищная нагрузка (1-5)
- ALT - высота над уровнем моря


<!-- <small>Источник: </small> -->

## Модель из прошлой лекции

```{r}
bird <- read.csv("data/loyn.csv")
bird$logAREA <- log(bird$AREA)
bird$logDIST <- log(bird$DIST)
bird$logLDIST <- log(bird$LDIST)
mod2 <- lm(ABUND ~ logAREA + YRISOL + logDIST + logLDIST + ALT, data = bird)
```


## Влияют ли предикторы?

```{r}
summary(mod2)
```

Не все предикторы влияют, возможно, эту модель можно оптимизировать...

# Сравнение линейных моделей

# Вложенные модели (nested models)

## Вложенные модели (nested models)

Две модели являются _вложенными_, если одну из них можно получить из другой путем удаления некоторых предикторов.   

Удаление предиктора  - коэффициент при данном предикторе равен нулю. 

### Полная модель (full model)

М1: $y _i = \beta _0 + \beta _1 x _1 + \beta _2 x _2 + \epsilon _i$

### Неполные модели (reduced models)

М2: $y _i = \beta _0 + \beta _1 x _1 + \epsilon _i$   

М3: $y _i = \beta _0 + \beta _2 x _2 + \epsilon _i$

M2 вложена в M1   
M3 вложена в M1   
M2 и M3 не вложены друг в друга

### Нулевая модель (null model), вложена в полную (M1) и в неполные (M2, M3)

$y _i = \beta _0 + \epsilon _i$

## Задание

Для тренировки запишем вложенные модели для данной полной модели

(1)$y _i = \beta _0 + \beta _1 x _1 + \beta _2 x _2 + \beta _3 x _3 + \epsilon _i$

## Решение

Для тренировки запишем вложенные модели для данной полной модели

(1)$y _i = \beta _0 + \beta _1 x _1 + \beta _2 x _2 + \beta _3 x _3 + \epsilon _i$

<div class="columns-2">

Модели:

- (2)$y _i = \beta _0 + \beta _1 x _1 + \beta _2 x _2 + \epsilon _i$
- (3)$y _i = \beta _0 + \beta _1 x _1 + \beta _3 x _3 + \epsilon _i$
- (4)$y _i = \beta _0 + \beta _2 x _2 + \beta _3 x _3 + \epsilon _i$
- (5)$y _i = \beta _0 + \beta _1 x _1 + \epsilon _i$
- (6)$y _i = \beta _0 + \beta _2 x _2 + \epsilon _i$
- (7)$y _i = \beta _0 + \beta _3 x _3 + \epsilon _i$
- (8)$y _i = \beta _0 + \epsilon _i$<br /><br />

Вложенность:

- (2)-(4)- вложены в (1)<br /><br /><br />
- (5)-(7)- вложены в (1), при этом 
   - (5)вложена в (1), (2), (3); 
   - (6)вложена в (1), (2), (4); 
   - (7)вложена в (1), (3), (4)<br /><br />
- (8)- нулевая модель - вложена во все

</div>

# Частный F-критерий

## Сравнение вложенных линейных моделей при помощи F-критерия

### Полная модель 

$y _i = \beta _0 + \beta _1 x _{i1} + \ldots + \beta _k x _{ik} + \beta _p x _{ip} + \epsilon _i$  
$df _{reduced, full} = p$  
$df _{error, full} = n - p - 1$

### Уменьшеная модель  (без фактора $\beta _p x _{ip}$)

$y _i = \beta _0 + \beta _1 x _{i1} + \ldots + \beta _k x _{ik} + \epsilon _i$  
$df _{reduced, reduced} = k$  
$df _{error, reduced} = n - k - 1$

### Как оценить насколько больше изменчивости объясняет полная модель, чем уменьшенная модель?

>- Разница объясненной изменчивости --- $SS _{error,reduced} - SS _{error,full}$ 
>- С чем, по аналогии с обычным F-критерием, можно сравнить эту разницу объясненной изменчивости?
>- Можно сравнить с остаточной изменчивостью полной модели --- $SS _{error, full}$

## Сравнение вложенных линейных моделей при помощи F-критерия

### Полная модель 

$y _i = \beta _0 + \beta _1 x _{i1} + \ldots + \beta _k x _{ik} + \beta _p x _{ip} + \epsilon _i$  
$df _{reduced, full} = p$  
$df _{error, full} = n - p - 1$

### Уменьшеная модель  (без фактора $\beta _p x _{ip}$)

$y _i = \beta _0 + \beta _1 x _{i1} + \ldots + \beta _k x _{ik} + \epsilon _i$  
$df _{reduced, reduced} = k$  
$df _{error, reduced} = n - k - 1$

### Частный F-критерий - оценивает выигрыш объясненной дисперсии от включения фактора в модель

$$F = \frac {(SS _{error,reduced} - SS _{error,full}) / (df _{reduced, full} - df _{reduced, reduced})} {(SS _{error, full})/ df _{error, full}}$$

## Сравнение линейных моделей при помощи частного F-критерия

Постепенно удаляем предикторы. Модели обязательно должны быть вложенными! *

### Обратный пошаговый алгоритм (backward selection)

>- - 1.Подбираем полную модель

>- Повторяем 2-3 для каждого из предикторов:  
- 2.Удаляем один предиктор (строим уменьшенную модель)  
- 3.Тестируем отличие уменьшенной модели от полной

>- 4.Выбираем предиктор для окончательного удаления: это предиктор, удаление которого минимально ухудшает модель. Модель без него будет "полной" для следующего раунда выбора оптимальной модели.  

>- Повторяем 1-4 до тех пор, пока что-то можно удалить.

<hr/>
* --- __Важно!__ Начинать упрощать модель нужно со взаимодействий между предикторами.  Если взаимодействие из модели удалить нельзя, то нельзя удалять и отдельно стоящие предикторы, из которых оно состоит. Но мы поговорим о взаимодействиях позже

# Частный F-критерий в R

## Влияют ли предикторы?

>- Можем попробовать оптимизировать модель

```{r}
summary(mod2)
```

## Частный F-критерий, 1 способ: `anova(модель_1, модель_2)`

Вручную выполняем все действия и выбираем, что можно выкинуть, и так много раз.

```{r eval=FALSE}
mod3 <- update(mod2, . ~ . - logAREA)
anova(mod2, mod3)
mod4 <- update(mod3, . ~ . - YRISOL)
anova(mod2, mod4)
mod5 <- update(mod3, . ~ . - logDIST)
anova(mod2, mod5)
mod6 <- update(mod3, . ~ . - logLDIST)
anova(mod2, mod6)
mod7 <- update(mod3, . ~ . - ALT)
anova(mod2, mod7)
# Удаляем один, и потом повторяем все снова...
```

Но мы пойдем другим путем

##  Частный F-критерий, 2 способ: `drop1()`

Вручную тестировать каждый предиктор с помощью `anova()` слишком долго. Можно протестировать все за один раз при помощи `drop1()`

```{r}
drop1(mod2, test = "F")
# Нужно убрать logDIST
```

## Тестируем предикторы (шаг 2)

```{r}
# Убрали logDIST
mod3 <- update(mod2, . ~ . - logDIST)
drop1(mod3, test = "F")
# Нужно убрать logLDIST 
```

## Тестируем предикторы (шаг 3)

```{r purl=FALSE}
# Убрали logLDIST 
mod4 <- update(mod3, . ~ . - logLDIST )
drop1(mod4, test = "F")
# Нужно убрать ALT
```

## Тестируем предикторы (шаг 4)

```{r purl=FALSE}
# Убрали ALT
mod5 <- update(mod4, . ~ . - ALT)
drop1(mod5, test = "F")
# Больше ничего не убрать
```

## Итоговая модель

```{r}
summary(mod5)
```

## Задание

Проверьте финальную модель на выполнение условий применимости

## Решение

### 1) График расстояния Кука 

- Выбросов нет

```{r solution-0a, fig.show='hold', purl=FALSE, fig.width=10, fig.height=2.2}
mod5_diag <- data.frame(
  fortify(mod5), 
  bird[, c("logDIST", "logLDIST", "GRAZE", "ALT")]
  )

ggplot(mod5_diag, aes(x = 1:nrow(mod5_diag), y = .cooksd)) + 
  geom_bar(stat = "identity")
```

## Решение

### 2) График остатков от предсказанных значений

- Выбросов нет
- Гетерогенность дисперсии?

```{r solution-1a, fig.show='hold', purl=FALSE, fig.width=10, fig.height=2.2}
gg_resid <- ggplot(data = mod5_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + geom_hline(yintercept = 0)
gg_resid
```


## Решение

### 3) Графики остатков от предикторов в модели и нет

```{r solution-2a, fig.show='hold', purl=FALSE, fig.width=10, fig.height=5, echo=FALSE}
res_1 <- gg_resid + aes(x = logAREA)
res_2 <- gg_resid + aes(x = YRISOL)
res_3 <- gg_resid + aes(x = logDIST)
res_4 <- gg_resid + aes(x = logLDIST)
res_5 <- gg_resid + aes(x = GRAZE)
res_6 <- gg_resid + aes(x = ALT)

library(gridExtra)
grid.arrange(res_1, res_2, res_3, res_4,
             res_5, res_6, nrow = 2)
```

## Решение

### 3) Код для графиков остатков от предикторов в модели и нет

```{r solution-2a, fig.show='hide', purl=FALSE, echo=TRUE}
```

## Решение

### 4) Квантильный график остатков

- Отклонения от нормального распределения остатков не выявляются

```{r solution-3a, purl=FALSE, fig.width=4, fig.height=4}
library(car)
qqPlot(mod5)
```


## Описываем финальную модель

```{r}
summary(mod5)
```

# Тесты отношения правдоподобий

## Вероятность и правдоподобие

Правдоподобие (likelihood) ---  способ измерить соответствие имеющихся данных тому, что можно получить при определенных значениях параметров модели.

Мы оцениваем это как произведение вероятностей получения каждой из точек данных

$$L(\theta| data) = \Pi^n _{i = 1}f(x| \theta)$$

где $f(data| \theta)$ - функция плотности распределения с параметрами $\theta$

```{r gg-norm-tunnel, echo=FALSE, fig.height=4, purl=FALSE}
## Based on code by Arthur Charpentier:
## http://freakonometrics.hypotheses.org/9593
## TODO: wrap it into a function and adapt it for use with other distributions
## as Markus Gesmann has done here
## http://www.magesblog.com/2015/08/visualising-theoretical-distributions.html
set.seed(32981)
op <- par(mar = c(0, 0, 0, 0))
n <- 2
X <- runif(25, 0, 10)
Y <- 5 + 1.2 * X + rnorm(25, 0, 5)
df <- data.frame(X,Y)

# regression
reggig <- glm(Y ~ X, data = df, family = gaussian(link = "identity"))

# empty plot
vX <- seq(min(X) - 0.1, max(X) + 0.1, length = n)
dy <- 2 * sd(Y)
vY <- seq(min(Y) - dy, max(Y) + dy, length = n)
mat <- persp(x = vX, y = vY, z = matrix(0, n, n),
             zlim = c(0, 0.03),
             theta =  -40, phi = 15, expand = 0.1,
             ticktype  = "detailed",  box = FALSE, border = "gray60")

x <- seq(min(X), max(X), length = 501)

# expected values
C <- trans3d(x, predict(reggig, newdata = data.frame(X = x), type = "response"), rep(0, length(x)), mat)
lines(C, lwd = 2)

sdgig <- sqrt(summary(reggig)$dispersion)

# 1SD
y1 <- qnorm(.95, predict(reggig, newdata = data.frame(X = x), type = "response"),  sdgig)
C <- trans3d(x, y1, rep(0, length(x)), mat)
lines(C, lty = 2, col = "#d95f02")
y2 <- qnorm(.05, predict(reggig, newdata = data.frame(X = x), type = "response"),  sdgig)
C <- trans3d(x, y2, rep(0, length(x)), mat)
lines(C, lty = 2, col = "#d95f02")

# C <- trans3d(c(x, rev(x)), c(y1, rev(y2)), rep(0, 2 * length(x)), mat)
# polygon(C, border = NA, col = "yellow")

# data points
C <- trans3d(X, Y, rep(0, length(X)), mat)
points(C, pch = 1, col = "black", cex = 0.4)

# density curves
n <- 6
vX <- seq(min(X), max(X), length = n)

mgig <- predict(reggig, newdata = data.frame(X = vX))

sdgig <- sqrt(summary(reggig)$dispersion)

for(j in n:1){
  stp <- 251
  x <- rep(vX[j], stp)
  y <- seq(min(min(Y) - dy, 
               qnorm(.05, 
                     predict(reggig, 
                             newdata = data.frame(X = vX[j]), 
                             type = "response"),  
                     sdgig)), 
           max(Y) + dy, 
           length = stp)
  z0 <- rep(0, stp)
  z <- dnorm(y,  mgig[j],  sdgig)
  C <- trans3d(c(x, x), c(y, rev(y)), c(z, z0), mat)
  polygon(C, border = NA, col = "light blue", density = 40)
  C <- trans3d(x, y, z0, mat)
  lines(C, lty = 2, col = "grey60")
  C <- trans3d(x, y, z, mat)
  lines(C, col = "steelblue")
}
par(op)
```


## Выводим формулу правдоподобия для линейной модели с нормальным распределением ошибок

$y_i = \beta_0 + \beta_1x_1 + \ldots + \beta_kx_k + \epsilon_i$

## Выводим формулу правдоподобия для линейной модели с нормальным распределением ошибок

$y_i = \beta_0 + \beta_1x_1 + \ldots + \beta_kx_k + \epsilon_i$

Пусть в нашей модели остатки нормально распределены ($\epsilon_i \sim N(0, \sigma^2)$) и их значения независимы друг от друга:

$N(\epsilon_i; 0, \sigma^2) = \frac {1} { \sqrt {2\pi\sigma^2} } exp (-\frac {1} {2 \sigma^2} \epsilon_i^2)$

## Выводим формулу правдоподобия для линейной модели с нормальным распределением ошибок

$y_i = \beta_0 + \beta_1x_1 + \ldots + \beta_kx_k + \epsilon_i$

Пусть в нашей модели остатки нормально распределены ($\epsilon_i \sim N(0, \sigma^2)$) и их значения независимы друг от друга:

$N(\epsilon_i; 0, \sigma^2) = \frac {1} { \sqrt {2\pi\sigma^2} } exp (-\frac {1} {2 \sigma^2} \epsilon_i^2)$

Функцию правдоподобия (likelihood, вероятность получения нашего набора данных) можно записать как произведение вероятностей:

$L(\epsilon_i|\mathbf{y}, \mathbf{x}) = \Pi^n _{n = 1} N(\epsilon_i, \sigma^2) = \frac {1} {\sqrt{(2\pi\sigma^2)^n}} exp(- \frac {1} {2\sigma^2} \sum {\epsilon_i}^2)$

## Выводим формулу правдоподобия для линейной модели с нормальным распределением ошибок

$y_i = \beta_0 + \beta_1x_1 + \ldots + \beta_kx_k + \epsilon_i$

Пусть в нашей модели остатки нормально распределены ($\epsilon_i \sim N(0, \sigma^2)$) и их значения независимы друг от друга:

$N(\epsilon_i; 0, \sigma^2) = \frac {1} { \sqrt {2\pi\sigma^2} } exp (-\frac {1} {2 \sigma^2} \epsilon_i^2)$

Функцию правдоподобия (likelihood, вероятность получения нашего набора данных) можно записать как произведение вероятностей:

$L(\epsilon_i|\mathbf{y}, \mathbf{x}) = \Pi^n _{n = 1} N(\epsilon_i, \sigma^2) = \frac {1} {\sqrt{(2\pi\sigma^2)^n}} exp(- \frac {1} {2\sigma^2} \sum {\epsilon_i}^2)$

Поскольку $\epsilon_i = y_i - (\beta_0 + \beta_1x_1 + \ldots + \beta_kx_k)$

то функцию правдоподобия можно переписать так:

$L(\beta_1\ldots\beta_k, \sigma^2| \mathbf{y}, \mathbf{x}) = \frac {1} {\sqrt{(2\pi\sigma^2)^n}}exp(- \frac {1} {2\sigma^2} \sum (y_i - (\beta_0 + \beta_1x_1 + \ldots + \beta_kx_k))^2)$

## Подбор параметров модели методом максимального правдоподобия

Чтобы найти параметры модели

$$y_i = \beta_0 + \beta_1x_1 + \ldots + \beta_kx_k + \epsilon_i$$

нужно найти такое сочетание параметров  $\beta_0$, $\beta_1$, \ldots $\beta_k$, и $\sigma^2$, при котором функция правдоподобия будет иметь максимум (именно поэтому метод называется метод максимального правдоподобия:

$\begin{array}{l}
L(\beta_1\ldots\beta_k, \sigma^2| \mathbf{y}, \mathbf{x}) &= \frac {1} {\sqrt{(2\pi\sigma^2)^n}} exp(- \frac {1} {2\sigma^2} \sum {\epsilon_i}^2) = \\
&= \frac {1} {\sqrt{(2\pi\sigma^2)^n}}exp(- \frac {1} {2\sigma^2} \sum (y_i - (\beta_0 + \beta_1x_1 + \ldots + \beta_kx_k))^2)
\end{array}$


## Логарифм правдоподобия (loglikelihood)

Вычислительно проще работать с логарифмами правдоподобий (loglikelihood)

Если функция правдоподобия

$\begin{array}{l}
L(\beta_1\ldots\beta_k, \sigma^2| \mathbf{y}, \mathbf{x}) &= \frac {1} {\sqrt{(2\pi\sigma^2)^n}} exp(- \frac {1} {2\sigma^2} \sum {\epsilon_i}^2) = \\
&= \frac {1} {\sqrt{(2\pi\sigma^2)^n}}exp(- \frac {1} {2\sigma^2} \sum (y_i - (\beta_0 + \beta_1x_1 + \ldots + \beta_kx_k))^2)
\end{array}$

то логарифм правдоподобия

$\begin{array}{l}
logLik(\beta_1\ldots\beta_k, \sigma^2| \mathbf{y}, \mathbf{x}) &= & \\
ln L(\beta_1\ldots\beta_k, \sigma^2| \mathbf{y}, \mathbf{x}) &= &- \frac{n}{2} (ln2\pi + ln\sigma^2) - \frac{1}{2\sigma^2}(\sum \epsilon^2_i) = \\
&= &- \frac{n}{2} (ln2\pi + ln\sigma^2) - \\
& &- \frac{1}{2\sigma^2}(\sum (y_i - (\beta_0 + \beta_1x_1 + \ldots + \beta_kx_k))^2)
\end{array}$

Чем больше логарифм правдоподобия тем лучше модель.

## Подбор параметров модели методом максимального правдоподобия

Для подбора параметров методом максимального правдоподобия используют функцию `glm()`


```{r}
# Симулированные данные
set.seed(9328)
dat <- data.frame(X = runif(n = 50, min = 0, max = 10))
dat$Y <- 3 + 15 * dat$X + rnorm(n = 50, mean = 0, sd = 1)

# Подбор модели двумя способами
Mod     <-  lm(Y ~ X, data = dat) # МНК
Mod_glm <- glm(Y ~ X, data = dat) # МЛ

# Одинаковые оценки коэффициентов
coefficients(Mod)
coefficients(Mod_glm)
```


## Логарифм правдоподобия

$LogLik$ для модели можно найти с помощью функции `logLic()`

```{r}
logLik(Mod_glm)
```

## Логарифм правдоподобия вручную {.smaller}


```{r}
# Предсказанные значения
dat$predicted <- predict(Mod)
# Оценка дисперсии
SD <- summary(Mod)$sigma 
# Вероятности для каждой точки
dat$Prob <- dnorm(dat$Y, mean = dat$predicted, sd = SD)
# Логарифм вероятностей
dat$LogProb <- log(dat$Prob)
# Логарифм произведения, равный сумме логарифмов
sum(dat$LogProb)
```


## Тест отношения правдоподобий (Likelihood Ratio Test)

Тест отношения правдоподобий позволяет определить какая модель более правдоподобна с учетом данных.

$$LRT = 2ln\Big(\frac{L_1}{L_2}\Big) = 2(lnL_1 - lnL_2)$$

- $L_1$, $L_2$ - правдоподобия полной и уменьшенной модели
- $lnL_1$, $lnL_2$ - логарифмы правдоподобий

Разница логарифмов правдоподобий имеет распределение, которое можно апроксимировать $\chi^2$, с числом степеней свободы $df = df_2 - df_1$ (Wilks, 1938)

# Тест отношения правдоподобий в R

## Задание

Для этой полной модели

```{r}
GLM1 <- glm(ABUND ~ logAREA + YRISOL + logDIST + logLDIST + ALT, data = bird)
```

Подберите оптимальную модель при помощи тестов отношения правдоподобий

Тест отношения правдоподобий можно сделать с помощью тех же функций, что и частный F-критерий:

- по-одному `anova(mod3, mod2, test = "Chisq")`
- все сразу `drop1(mod3, test = "Chisq")`


## Решение (шаг 1)

```{r purl=FALSE}
drop1(GLM1, test = "Chisq")
# Нужно убрать logDIST
```

## Решение (шаг 2)

```{r purl=FALSE}
# Убираем logDIST
GLM2 <- update(GLM1, . ~ . - logDIST)
drop1(GLM2, test = "Chisq")
# Нужно убрать logLDIST
```

## Решение (шаг 3)

```{r purl=FALSE}
# Убираем logLDIST
GLM3 <- update(GLM2, . ~ . - logLDIST)
drop1(GLM3, test = "Chisq")
# Нужно убрать ALT
```

## Решение (шаг 4)

```{r purl=FALSE}
# Убираем ALT
GLM4 <- update(GLM3, . ~ . - ALT)
drop1(GLM4, test = "Chisq")
# Больше ничего убрать не получается
```

## Решение (шаг 5)

```{r purl=FALSE}
summary(GLM4)
```

Финальную модель нужно проверить на выполнение условий применимости

# Информационные критерии

## AIC - Информационный критерий Акаике (Akaike Information Criterion)

$AIC = -2 logLik + 2p$

- $logLik$ - логарифм правдоподобия для модели
- $2p$ - штраф за введение в модель $p$ параметров, т.е. за "сложность" модели

AIC --- это мера потери информации, которая происходит, если реальность описывать этой моделью (Akaike 1974)

AIC --- относительная мера качества модели. Т.е. не бывает какого-то "хорошего" AIC. Значения AIC можно интерпретировать только в сравнении с AIC для других моделей: чем меньше AIC --- тем лучше модель.

__Важно!__   Информационные критерии можно использовать для сравнения __даже для невложенных моделей__. Но модели должны быть __подобраны с помощью ML__ и __на одинаковых данных__!

## Некоторые другие информационные критерии

|Критерий | Название  | Формула|
|------ | ------ | ------|
|AIC | Информационный критерий Акаике | $AIC = -2 logLik + 2p$|
|BIC | Баесовский информационный критерий | $BIC = -2 logLik + p \cdot ln(n)$|
|AICc | Информационный критерий Акаике с коррекцией для малых выборок (малых относительно числа параметров: $n/p < 40$, Burnham, Anderson, 2004) | $AIC_c = -2 logLik + 2p + \frac{2p(p + 1)}{n - p - 1}$|

- $logLik$ - логарифм правдоподобия для модели
- $p$ - число параметров
- $n$ - число наблюдений


# Информационные критерии в R

```{r echo=FALSE, purl=FALSE}
lm_equation <- function(fit, strict = TRUE, rnd = 2){
#   extracting call formula 
  frml <- as.character(fit$call)[2]
#   extract signs
    sign <- ifelse(grepl("-", coef(fit)[-1]), " - ", " + ")
  # extract coefficients
  coeffs <- format(round(abs(coef(fit)), rnd), digits = 2, nsmall = rnd, trim = TRUE)
  if(strict == TRUE){
    i <- 1:(length(coeffs) - 1)
    vars <- c("Y", paste0(" X", i))
    
  } else {
# extract vector of variable names
  vars <- c(all.vars(formula(fit))[1], names(fit$coefficients)[-1])
# combine everything
  }
  start <- ifelse(coef(fit)[1] > 0, paste(vars[1], coeffs[1], sep = " = "), paste(vars[1], coeffs[1], sep = " = - "))
  end <- paste(sign, coeffs[-1], vars[-1], sep = "", collapse = "")
  return(paste0(start, end, sep = ""))
}
```

## Рассчитаем AIC для наших моделей

```{r}
AIC(GLM1, GLM2, GLM3, GLM4)
```

>- Судя по AIC, лучшая модель GLM3 или GLM4. Если значения AIC различаются всего на 1-2 единицу --- такими различиями можно пренебречь и выбрать более простую модель (GLM4).

Уравнение модели:

$`r lm_equation(GLM4, strict = FALSE)`$


## Как выбрать способ подбора оптимальной модели?

Вы видели, что разные способы подбора оптимальной модели могут приводить к разным результатам.

Не важно, какой из способов выбрать, но важно сделать это заранее, __до анализа__, чтобы не поддаваться соблазну подгонять результаты.

## Take-home messages

- Модели, которые качественно описывают существующие данные включают много параметров, но предсказания с их помощью менее точны из-за переобучения
- Для выбора оптимальной модели используются разные критерии в зависимости от задачи
- Сравнивая модели можно отбраковать переменные, включение которых в модель не улучшает ее
- __Метод сравнения моделей нужно выбрать заранее, еще до анализа__


## Что почитать

+ <span style="color:red">Must read paper!</span> Zuur, A.F. and Ieno, E.N., 2016. A protocol for conducting and presenting results of regression‐type analyses. Methods in Ecology and Evolution, 7(6), pp.636-645.

+ Кабаков Р.И. R в действии. Анализ и визуализация данных на языке R. М.: ДМК Пресс, 2014
+ Zuur, A., Ieno, E.N. and Smith, G.M., 2007. Analyzing ecological data. Springer Science & Business Media.
+ Quinn G.P., Keough M.J. 2002. Experimental design and data analysis for biologists
+ Logan M. 2010. Biostatistical Design and Analysis Using R. A Practical Guide
